{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f4da9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"/tmp/tmpf7chzbnf\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter\n",
    "\n",
    "from nvcc4jupyter import set_defaults\n",
    "set_defaults(compiler_args='-arch=sm_100a -Xptxas=-v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f2be4",
   "metadata": {},
   "source": [
    "We are so fucking back, have a sore throat, IM PISSED AND IN NEED TO CODE AND THINK IMMA GO TAKE SOME COUGHT SUPRESSSANT AND BE RIGHT BACK. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5571ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa93e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 0.000000, output: 1.000000 \n",
      "input: 0.001000, output: 1.001000 \n",
      "input: 0.002000, output: 1.002000 \n",
      "input: 0.003000, output: 1.003000 \n",
      "input: 0.004000, output: 1.004000 \n",
      "input: 0.005000, output: 1.005000 \n",
      "input: 0.006000, output: 1.006000 \n",
      "input: 0.007000, output: 1.007000 \n",
      "input: 0.008000, output: 1.008000 \n",
      "input: 0.009000, output: 1.009000 \n",
      "input: 0.010000, output: 1.010000 \n",
      "input: 0.011000, output: 1.011000 \n",
      "input: 0.012000, output: 1.012000 \n",
      "input: 0.013000, output: 1.013000 \n",
      "input: 0.014000, output: 1.014000 \n",
      "input: 0.015000, output: 1.015000 \n",
      "input: 0.016000, output: 1.016000 \n",
      "input: 0.017000, output: 1.017000 \n",
      "input: 0.018000, output: 1.018000 \n",
      "input: 0.019000, output: 1.019000 \n",
      "input: 0.020000, output: 1.020000 \n",
      "input: 0.021000, output: 1.021000 \n",
      "input: 0.022000, output: 1.022000 \n",
      "input: 0.023000, output: 1.023000 \n",
      "input: 0.024000, output: 1.024000 \n",
      "input: 0.025000, output: 1.025000 \n",
      "input: 0.026000, output: 1.026000 \n",
      "input: 0.027000, output: 1.027000 \n",
      "input: 0.028000, output: 1.028000 \n",
      "input: 0.029000, output: 1.029000 \n",
      "input: 0.030000, output: 1.030000 \n",
      "input: 0.031000, output: 1.031000 \n",
      "input: 0.032000, output: 1.032000 \n",
      "input: 0.033000, output: 1.033000 \n",
      "input: 0.034000, output: 1.034000 \n",
      "input: 0.035000, output: 1.035000 \n",
      "input: 0.036000, output: 1.036000 \n",
      "input: 0.037000, output: 1.037000 \n",
      "input: 0.038000, output: 1.038000 \n",
      "input: 0.039000, output: 1.039000 \n",
      "input: 0.040000, output: 1.040000 \n",
      "input: 0.041000, output: 1.041000 \n",
      "input: 0.042000, output: 1.042000 \n",
      "input: 0.043000, output: 1.043000 \n",
      "input: 0.044000, output: 1.044000 \n",
      "input: 0.045000, output: 1.045000 \n",
      "input: 0.046000, output: 1.046000 \n",
      "input: 0.047000, output: 1.047000 \n",
      "input: 0.048000, output: 1.048000 \n",
      "input: 0.049000, output: 1.049000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include<stdio.h>\n",
    "#include<stdlib.h>\n",
    "#include<cuda.h> \n",
    "#include<cuda_runtime.h> \n",
    "#include<cooperative_groups.h> \n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "\n",
    "constexpr int N = 4096; \n",
    "constexpr int block_size = 1024; \n",
    "constexpr int N_blocks = 1; \n",
    "constexpr int N_iter = N/block_size;\n",
    "\n",
    "__global__ void three_step_sync(float*A, float*B)\n",
    "{\n",
    "  __shared__ float S[block_size]; \n",
    "  auto b = cg::this_thread_block(); \n",
    "  int t = b.thread_rank();\n",
    "  float r = 0.0;\n",
    "  for (int i = 0; i < N_iter; i++)\n",
    "  {\n",
    "    //#loads into shared memory\n",
    "    S[t] = A[i+t]; \n",
    "    b.sync(); //#creates a barrier where each of the threads wait until all the other threads are complete\n",
    "    r = S[t] + 1.0;\n",
    "    B[i+t] = r;\n",
    "    b.sync();//#another barrier for each thread to be done writing results into gmem and then \n",
    "    //#and only then does the next loop start again\n",
    "    \n",
    "  }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  float *A_h, *A_d, *B_h, *B_d; \n",
    "  size_t size = N*sizeof(float);\n",
    "  \n",
    "  cudaHostAlloc(&A_h, size, cudaHostAllocDefault);\n",
    "  cudaHostAlloc(&B_h, size, cudaHostAllocDefault); \n",
    "  \n",
    "  cudaMalloc(&A_d, size);\n",
    "  cudaMalloc(&B_d, size);\n",
    "  \n",
    "  for (int i = 0; i < N; i++)\n",
    "  {\n",
    "    A_h[i] = (float)(i/1000.0);\n",
    "  }\n",
    "  \n",
    "  cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice); \n",
    "  \n",
    "  three_step_sync<<<N_blocks, block_size>>>(A_d, B_d); \n",
    "  \n",
    "  cudaDeviceSynchronize();\n",
    "  \n",
    "  cudaMemcpy(B_h, B_d, size, cudaMemcpyDeviceToHost);\n",
    "  \n",
    "  for (int i = 0; i < 50; i++)\n",
    "  {\n",
    "    printf(\"input: %f, output: %f \\n\", A_h[i], B_h[i]);\n",
    "  }\n",
    "  \n",
    "  cudaFree(A_d);\n",
    "  cudaFree(B_d);\n",
    "  cudaFreeHost(A_h); \n",
    "  cudaFreeHost(B_h);\n",
    "  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1a7b4",
   "metadata": {},
   "source": [
    "In the above, we have a basic 3 step sync pattern, if b is a block barrier,  there is, per thread, work before the wait point, that is, \n",
    "memory operations that each thread does independently, then at the wait point, each thread is blocked until the barrier counts down to all threads arriving there, then after the wait point is completed, the memory operations's results is visible to all threads. \n",
    "\n",
    "Next we will look at a more complex barrier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 0.000000, output: 0.000000 \n",
      "input: 0.001000, output: 0.000000 \n",
      "input: 0.002000, output: 0.000000 \n",
      "input: 0.003000, output: 0.000000 \n",
      "input: 0.004000, output: 0.000000 \n",
      "input: 0.005000, output: 0.000000 \n",
      "input: 0.006000, output: 0.000000 \n",
      "input: 0.007000, output: 0.000000 \n",
      "input: 0.008000, output: 0.000000 \n",
      "input: 0.009000, output: 0.000000 \n",
      "input: 0.010000, output: 0.000000 \n",
      "input: 0.011000, output: 0.000000 \n",
      "input: 0.012000, output: 0.000000 \n",
      "input: 0.013000, output: 0.000000 \n",
      "input: 0.014000, output: 0.000000 \n",
      "input: 0.015000, output: 0.000000 \n",
      "input: 0.016000, output: 0.000000 \n",
      "input: 0.017000, output: 0.000000 \n",
      "input: 0.018000, output: 0.000000 \n",
      "input: 0.019000, output: 0.000000 \n",
      "input: 0.020000, output: 0.000000 \n",
      "input: 0.021000, output: 0.000000 \n",
      "input: 0.022000, output: 0.000000 \n",
      "input: 0.023000, output: 0.000000 \n",
      "input: 0.024000, output: 0.000000 \n",
      "input: 0.025000, output: 0.000000 \n",
      "input: 0.026000, output: 0.000000 \n",
      "input: 0.027000, output: 0.000000 \n",
      "input: 0.028000, output: 0.000000 \n",
      "input: 0.029000, output: 0.000000 \n",
      "input: 0.030000, output: 0.000000 \n",
      "input: 0.031000, output: 0.000000 \n",
      "input: 0.032000, output: 0.000000 \n",
      "input: 0.033000, output: 0.000000 \n",
      "input: 0.034000, output: 0.000000 \n",
      "input: 0.035000, output: 0.000000 \n",
      "input: 0.036000, output: 0.000000 \n",
      "input: 0.037000, output: 0.000000 \n",
      "input: 0.038000, output: 0.000000 \n",
      "input: 0.039000, output: 0.000000 \n",
      "input: 0.040000, output: 0.000000 \n",
      "input: 0.041000, output: 0.000000 \n",
      "input: 0.042000, output: 0.000000 \n",
      "input: 0.043000, output: 0.000000 \n",
      "input: 0.044000, output: 0.000000 \n",
      "input: 0.045000, output: 0.000000 \n",
      "input: 0.046000, output: 0.000000 \n",
      "input: 0.047000, output: 0.000000 \n",
      "input: 0.048000, output: 0.000000 \n",
      "input: 0.049000, output: 0.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include<stdio.h>\n",
    "#include<stdlib.h>\n",
    "#include<cuda.h> \n",
    "#include<cuda_runtime.h> \n",
    "#include <cuda/barrier>\n",
    "#include <cooperative_groups.h>\n",
    "\n",
    "namespace cg = cooperative_groups;\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "\n",
    "constexpr int N = 4096; \n",
    "constexpr int block_size = 1024; \n",
    "constexpr int N_blocks = 1; \n",
    "constexpr int TM = N/block_size;\n",
    "\n",
    "__global__ void three_step_sync(float*A, float*B)\n",
    "{\n",
    "  __shared__ float S[block_size]; \n",
    "  auto b = cg::this_thread_block(); \n",
    "  int t = b.thread_rank();\n",
    "  float r = 0.0;\n",
    "  __shared__ barrier bar; //#create barrier on shared memory. \n",
    "  \n",
    "  //# To initalize a barrier, we need to make sure the threads are synchronized, and we initalize it with the expected arrival count\n",
    "  //# any single thread can init this barrier, the arrival count in our case is going to be block size. \n",
    "  if (t == 0)\n",
    "  {\n",
    "    init(&bar, b.size());\n",
    "\n",
    "  }\n",
    "  b.sync(); \n",
    "  //# at this point, thread 0 issued the init of the barrier instruction, and hence, things being issued on other threads \n",
    "  //# might happen paralell to the completetion of initing the barrier, therefore our threads are out of sync, and its hard for us to tell\n",
    "  //# when all the threads will arive at the barrier, therefore, we use b.sync() and just get all the threads in line :)\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  float *A_h, *A_d, *B_h, *B_d; \n",
    "  size_t size = N*sizeof(float);\n",
    "  \n",
    "  cudaHostAlloc(&A_h, size, cudaHostAllocDefault);\n",
    "  cudaHostAlloc(&B_h, size, cudaHostAllocDefault); \n",
    "  \n",
    "  cudaMalloc(&A_d, size);\n",
    "  cudaMalloc(&B_d, size);\n",
    "  \n",
    "  for (int i = 0; i < N; i++)\n",
    "  {\n",
    "    A_h[i] = (float)(i/1000.0);\n",
    "  }\n",
    "  \n",
    "  cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice); \n",
    "  \n",
    "  three_step_sync<<<N_blocks, block_size>>>(A_d, B_d); \n",
    "  \n",
    "  cudaDeviceSynchronize();\n",
    "  \n",
    "  cudaMemcpy(B_h, B_d, size, cudaMemcpyDeviceToHost);\n",
    "  \n",
    "  for (int i = 0; i < 50; i++)\n",
    "  {\n",
    "    printf(\"input: %f, output: %f \\n\", A_h[i], B_h[i]);\n",
    "  }\n",
    "  \n",
    "  cudaFree(A_d);\n",
    "  cudaFree(B_d);\n",
    "  cudaFreeHost(A_h); \n",
    "  cudaFreeHost(B_h);\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2647e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
