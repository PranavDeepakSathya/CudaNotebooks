{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c183e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"/tmp/tmp3k45waxx\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter\n",
    "\n",
    "from nvcc4jupyter import set_defaults\n",
    "set_defaults(compiler_args='-arch=sm_90a -Xptxas=-v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f81ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D[0]: 1 D[1]: 1 D[2]: 1 D[3]: 1 D[4]: 1 D[5]: 1 D[6]: 1 D[7]: 1 D[8]: 1 D[9]: 1 D[10]: 1 D[11]: 1 D[12]: 1 D[13]: 1 D[14]: 1 D[15]: 1 D[16]: 1 D[17]: 1 D[18]: 1 D[19]: 1 D[20]: 1 D[21]: 1 D[22]: 1 D[23]: 1 D[24]: 1 D[25]: 1 D[26]: 1 D[27]: 1 D[28]: 1 D[29]: 1 D[30]: 1 D[31]: 1 D[32]: 1 D[33]: 1 D[34]: 1 D[35]: 1 D[36]: 1 D[37]: 1 D[38]: 1 D[39]: 1 D[40]: 1 D[41]: 1 D[42]: 1 D[43]: 1 D[44]: 1 D[45]: 1 D[46]: 1 D[47]: 1 D[48]: 1 D[49]: 1 D[50]: 1 D[51]: 1 D[52]: 1 D[53]: 1 D[54]: 1 D[55]: 1 D[56]: 1 D[57]: 1 D[58]: 1 D[59]: 1 D[60]: 1 D[61]: 1 D[62]: 1 D[63]: 1 D[64]: 1 D[65]: 1 D[66]: 1 D[67]: 1 D[68]: 1 D[69]: 1 D[70]: 1 D[71]: 1 D[72]: 1 D[73]: 1 D[74]: 1 D[75]: 1 D[76]: 1 D[77]: 1 D[78]: 1 D[79]: 1 D[80]: 1 D[81]: 1 D[82]: 1 D[83]: 1 D[84]: 1 D[85]: 1 D[86]: 1 D[87]: 1 D[88]: 1 D[89]: 1 D[90]: 1 D[91]: 1 D[92]: 1 D[93]: 1 D[94]: 1 D[95]: 1 D[96]: 1 D[97]: 1 D[98]: 1 D[99]: 1 \n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include<stdio.h>\n",
    "#include<stdlib.h>\n",
    "#include<cuda_runtime.h>\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace ptx = cuda::ptx;\n",
    "\n",
    "static constexpr size_t buf_len = 1024;\n",
    "static constexpr int N_blocks = 4;\n",
    "static constexpr size_t data_len = N_blocks * buf_len;\n",
    "static constexpr int N_threads_per_block = 64;\n",
    "__global__ void add_one_kernel(int* data, size_t offset)\n",
    "{\n",
    "  //# Shared memory buffer. The destination shared memory buffer of\n",
    "  //# a bulk operations should be 16 byte aligned.\n",
    "  __shared__ alignas(16) int smem_data[buf_len];\n",
    "\n",
    "  //# 1. a) Initialize shared memory barrier with the number of threads participating in the barrier.\n",
    "  //#3    b) Make initialized barrier visible in async proxy.\n",
    "  #pragma nv_diag_suppress static_var_with_dynamic_init\n",
    "  __shared__ barrier bar;\n",
    "  if (threadIdx.x == 0) { \n",
    "    init(&bar, blockDim.x);                      // a)\n",
    "    ptx::fence_proxy_async(ptx::space_shared);   // b)\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  //# 2. Initiate TMA transfer to copy global to shared memory.\n",
    "  if (threadIdx.x == 0) {\n",
    "    //# 3a. cuda::memcpy_async arrives on the barrier and communicates\n",
    "    //#     how many bytes are expected to come in (the transaction count)\n",
    "    cuda::memcpy_async(\n",
    "        smem_data, \n",
    "        data + offset, \n",
    "        cuda::aligned_size_t<16>(sizeof(smem_data)),\n",
    "        bar\n",
    "    );\n",
    "  }\n",
    "  //# 3b. All threads arrive on the barrier\n",
    "  barrier::arrival_token token = bar.arrive();\n",
    "  \n",
    "  //# 3c. Wait for the data to have arrived.\n",
    "  bar.wait(std::move(token));\n",
    "\n",
    "  //# 4. Compute saxpy and write back to shared memory\n",
    "  for (int i = threadIdx.x; i < buf_len; i += blockDim.x) {\n",
    "    smem_data[i] += 1;\n",
    "  }\n",
    "\n",
    "  //# 5. Wait for shared memory writes to be visible to TMA engine.\n",
    "  ptx::fence_proxy_async(ptx::space_shared);   // b)\n",
    "  __syncthreads();\n",
    "  //# After syncthreads, writes by all threads are visible to TMA engine.\n",
    "\n",
    "  // #6. Initiate TMA transfer to copy shared memory to global memory\n",
    "  if (threadIdx.x == 0) {\n",
    "    ptx::cp_async_bulk(\n",
    "        ptx::space_global,\n",
    "        ptx::space_shared,\n",
    "        data + offset, smem_data, sizeof(smem_data));\n",
    "    // 7. Wait for TMA transfer to have finished reading shared memory.\n",
    "    // Create a \"bulk async-group\" out of the previous bulk copy operation.\n",
    "    ptx::cp_async_bulk_commit_group();\n",
    "    // Wait for the group to have completed reading from shared memory.\n",
    "    ptx::cp_async_bulk_wait_group_read(ptx::n32_t<0>());\n",
    "  }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  int*data_h; \n",
    "  cudaHostAlloc(&data_h, data_len, cudaHostAllocDefault); \n",
    "  int*data_d; \n",
    "  size_t offset = 0;\n",
    "  cudaMalloc(&data_d, data_len); \n",
    "  cudaMemcpy(data_d, data_h, data_len, cudaMemcpyHostToDevice); \n",
    "  add_one_kernel<<<N_blocks, N_threads_per_block>>>(data_d, offset);\n",
    "  cudaDeviceSynchronize(); \n",
    "  cudaMemcpy(data_h, data_d, data_len, cudaMemcpyDeviceToHost); \n",
    "  for (int i = 0; i < 100; i ++)\n",
    "  {\n",
    "    printf(\"D[%d]: %d \", i, data_h[i]);\n",
    "  }\n",
    "  cudaFreeHost(data_h); \n",
    "  cudaFree(data_d);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "203456fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include<stdio.h>\n",
    "#include<stdlib.h>\n",
    "#include<cuda_runtime.h>\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "#include<cuda.h>\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace ptx = cuda::ptx;\n",
    "#ifndef CUDA_UTILS_H\n",
    "#define CUDA_UTILS_H\n",
    "\n",
    "\n",
    "\n",
    "/**\n",
    " * @brief Macro to wrap CUDA API calls.\n",
    " * * Usage: CUDA_CHECK(cudaMalloc(&d_a, size));\n",
    " */\n",
    "#define CUDA_CHECK(call) \\\n",
    "    { \\\n",
    "        gpuAssert((call), __FILE__, __LINE__); \\\n",
    "    }\n",
    "\n",
    "/**\n",
    " * @brief Internal function to handle the error logic.\n",
    " * * @param code The error code returned by the CUDA function.\n",
    " * @param file The file name where the error occurred (provided by __FILE__).\n",
    " * @param line The line number where the error occurred (provided by __LINE__).\n",
    " * @param abort Whether to terminate the application on error (default: true).\n",
    " */\n",
    "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true)\n",
    "{\n",
    "    if (code != cudaSuccess) \n",
    "    {\n",
    "        // Print the error name (e.g., cudaErrorMemoryAllocation)\n",
    "        // and the description (e.g., \"out of memory\")\n",
    "        fprintf(stderr, \"CUDA_CHECK Error: %s %s\\n\", \n",
    "                cudaGetErrorName(code), \n",
    "                cudaGetErrorString(code));\n",
    "        \n",
    "        fprintf(stderr, \"  File: %s\\n\", file);\n",
    "        fprintf(stderr, \"  Line: %d\\n\", line);\n",
    "\n",
    "        if (abort) \n",
    "        {\n",
    "            // Optional: Reset device to flush profiling data before exit\n",
    "            cudaDeviceReset();\n",
    "            exit(code);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Macro to check for errors after a kernel launch.\n",
    " * * Kernel launches are asynchronous. This macro checks:\n",
    " * 1. If the launch itself failed (invalid configuration).\n",
    " * 2. Synchronizes the device to check for execution errors (bus errors, segfaults).\n",
    " * * WARNING: Using this slows down code significantly because of the synchronization.\n",
    " * Use only for debugging.\n",
    " */\n",
    "#define CUDA_CHECK_KERNEL() \\\n",
    "    { \\\n",
    "        cudaError_t err = cudaGetLastError(); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"Kernel Launch Error (Sync): %s\\n\", cudaGetErrorString(err)); \\\n",
    "            gpuAssert(err, __FILE__, __LINE__); \\\n",
    "        } \\\n",
    "        err = cudaDeviceSynchronize(); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"Kernel Execution Error (Async): %s\\n\", cudaGetErrorString(err)); \\\n",
    "            gpuAssert(err, __FILE__, __LINE__); \\\n",
    "        } \\\n",
    "    }\n",
    "\n",
    "#endif // CUDA_UTILS_H\n",
    "\n",
    "#include <cudaTypedefs.h> // PFN_cuTensorMapEncodeTiled, CUtensorMap\n",
    "\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "  // Get pointer to cuTensorMapEncodeTiled\n",
    "  cudaDriverEntryPointQueryResult driver_status;\n",
    "  void* cuTensorMapEncodeTiled_ptr = nullptr;\n",
    "  CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &cuTensorMapEncodeTiled_ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "  assert(driver_status == cudaDriverEntryPointSuccess);\n",
    "\n",
    "  return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(cuTensorMapEncodeTiled_ptr);\n",
    "}\n",
    "constexpr uint32_t N = 4096; \n",
    "constexpr uint32_t BN = 32;\n",
    "constexpr uint32_t rank = 2; \n",
    "constexpr uint32_t GMEM_WIDTH = N; \n",
    "constexpr uint32_t GMEM_HEIGHT = N; \n",
    "constexpr uint32_t SMEM_WIDTH = BN; \n",
    "constexpr uint32_t SMEM_HEIGHT = BN;\n",
    "uint64_t size[rank] = {GMEM_WIDTH, GMEM_HEIGHT}; //#the faster dimension is always left first. unit is number of elements\n",
    "uint64_t stride[rank-1] =  {GMEM_WIDTH*sizeof(float)}; //#the number of bytes to step from one row to the next, we are making a row major one here\n",
    "uint32_t box_size[rank] = {SMEM_WIDTH, SMEM_HEIGHT}; //# I wonder if the reduced stride is taken automatically, but \n",
    "//# our boys at NVDA say that things are assumed LINEAR in shared memory so I guess they don't give a fuck, but at this point \n",
    "//# the layout is already decided.\n",
    "uint32_t elem_stride[rank] = {1,1}; //# THIS VIDEO IS ALREADY TOO LONG FOR INSTAGRAM REEEEEELLLL FFS \n",
    "//# indeed, the element_strides are something along the lines of if each element is a row major 2 vector then {1,2} would be the elem stride\n",
    "//# but we have scalar elements so we guuci.\n",
    "\n",
    "int main()\n",
    "{\n",
    "  float* A_h, *A_d; \n",
    "  size_t size_tensor = N*N*sizeof(float);\n",
    "  cudaHostAlloc(&A_h, size_tensor, cudaHostAllocDefault); \n",
    "  cudaMalloc(&A_d, size_tensor); \n",
    "  void *tensor_ptr =  &A_d; \n",
    "  cudaMemcpy(A_d, A_h, size_tensor, cudaMemcpyHostToDevice);\n",
    "  \n",
    "  CUtensorMap tensor_map{};\n",
    "  auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled(); //#function pointer to the tensor map creation api\n",
    "  CUresult res = cuTensorMapEncodeTiled(\n",
    "  &tensor_map,                //# CUtensorMap *tensorMap,\n",
    "  CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_INT32,\n",
    "  rank,                       //# cuuint32_t tensorRank,\n",
    "  tensor_ptr,                 //# void *globalAddress,\n",
    "  size,                       //# const cuuint64_t *globalDim,\n",
    "  stride,                     //# const cuuint64_t *globalStrides,\n",
    "  box_size,                   //# const cuuint32_t *boxDim,\n",
    "  elem_stride,                //# const cuuint32_t *elementStrides,\n",
    "  //# Interleave patterns can be used to accelerate loading of values that\n",
    "  //# are less than 4 bytes long.\n",
    "  CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "  //# Swizzling can be used to avoid shared memory bank conflicts.\n",
    "  CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE,\n",
    "  //# L2 Promotion can be used to widen the effect of a cache-policy to a wider\n",
    "  //# set of L2 cache lines.\n",
    "  CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "  //# Any element that is outside of bounds will be set to zero by the TMA transfer.\n",
    "  CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    ");\n",
    "  \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d86ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
