{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28539697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"/tmp/tmp34l4zaxy\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter\n",
    "\n",
    "from nvcc4jupyter import set_defaults\n",
    "set_defaults(compiler_args='-arch=sm_90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cab8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection of First Row (0 to 15):\n",
      "Expected: 0, 1, 2, 3, 4, ...\n",
      "Actual  : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ...\n",
      "\n",
      "FAILURE: The data looks linear. Swizzle 128B was not effective.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <assert.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cudaTypedefs.h> \n",
    "#include <vector_types.h> \n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace cde = cuda::device::experimental;\n",
    "\n",
    "// -------------------------------------------------------------------------\n",
    "// CONFIGURATION\n",
    "// -------------------------------------------------------------------------\n",
    "// We use int4 (16 bytes). \n",
    "// 128 Byte Swizzle means the pattern repeats/shifts every 8 int4 elements (8 * 16 = 128).\n",
    "constexpr uint32_t M = 32; \n",
    "constexpr uint32_t N = 32; \n",
    "constexpr uint32_t BM = 32; \n",
    "\n",
    "// FIX: BN must result in a box width <= 128 bytes for SWIZZLE_128B.\n",
    "// int4 is 16 bytes. 128 / 16 = 8.\n",
    "// So BN must be <= 8.\n",
    "constexpr uint32_t BN = 8; \n",
    "\n",
    "constexpr uint32_t INT4_COMPONENTS = 4; // treating int4 as 4 ints for TMA\n",
    "\n",
    "// -------------------------------------------------------------------------\n",
    "// HOST HELPERS\n",
    "// -------------------------------------------------------------------------\n",
    "#define CUDA_CHECK(call) \\\n",
    "    { \\\n",
    "        cudaError_t err = (call); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n",
    "                    cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "            exit(err); \\\n",
    "        } \\\n",
    "    }\n",
    "\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "    cudaDriverEntryPointQueryResult driver_status;\n",
    "    void* ptr = nullptr;\n",
    "    CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "    return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(ptr);\n",
    "}\n",
    "\n",
    "// -------------------------------------------------------------------------\n",
    "// DEVICE KERNEL\n",
    "// -------------------------------------------------------------------------\n",
    "__global__ void show_swizzle_kernel(int4* Output_Buffer, const __grid_constant__ CUtensorMap tensor_map)\n",
    "{\n",
    "    // Standard linear coordinates\n",
    "    uint x_idx = blockIdx.x * blockDim.x; \n",
    "    uint y_idx = blockIdx.y * blockDim.y;\n",
    "    \n",
    "    // Shared Memory - 128B Aligned for TMA\n",
    "    // We pad the stride if necessary, but here BN*16 = 128 bytes, which is aligned.\n",
    "    __shared__ alignas(128) int4 As[BM][BN];  \n",
    "\n",
    "    #pragma nv_diag_suppress static_var_with_dynamic_init \n",
    "    __shared__ barrier bar;\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        init(&bar, BM * BN); \n",
    "        cde::fence_proxy_async_shared_cta(); \n",
    "    }\n",
    "    __syncthreads(); \n",
    "\n",
    "    // 1. TMA LOAD (SWIZZLED)\n",
    "    barrier::arrival_token token;\n",
    "    if (threadIdx.x == 0)\n",
    "    {\n",
    "        // Adjust X for INT32 view (int4 is 4 ints)\n",
    "        // Note: blockIdx.x * BN gives the element offset. \n",
    "        // We multiply by 4 to get the INT32 offset.\n",
    "        uint32_t tma_x = (blockIdx.x * BN) * INT4_COMPONENTS;\n",
    "        uint32_t tma_y = y_idx;\n",
    "\n",
    "        // The TMA engine will apply the XOR Swizzle here!\n",
    "        cde::cp_async_bulk_tensor_2d_global_to_shared(&As, &tensor_map, tma_x, tma_y, bar);\n",
    "        \n",
    "        token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(As));\n",
    "    } \n",
    "    else {\n",
    "        token = bar.arrive();\n",
    "    }\n",
    "\n",
    "    bar.wait(std::move(token));\n",
    "    cde::fence_proxy_async_shared_cta();\n",
    "    __syncthreads();\n",
    "\n",
    "    // 2. STANDARD READ (LINEAR)\n",
    "    // We read the shared memory \"As\" just like a normal array.\n",
    "    uint smem_col = threadIdx.x; \n",
    "    uint smem_row = threadIdx.y; \n",
    "    \n",
    "    // Write out to global memory linearly so we can inspect it on host\n",
    "    // Bounds check since we might have threads > BN if we reused the block size from M\n",
    "    if (smem_col < BN && smem_row < BM) {\n",
    "        uint gmem_index = (y_idx + smem_row) * N + (x_idx + smem_col);\n",
    "        Output_Buffer[gmem_index] = As[smem_row][smem_col];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    // Sizes\n",
    "    size_t size_bytes = M * N * sizeof(int4);\n",
    "    int4 *h_data, *d_data_in, *d_data_out;\n",
    "    \n",
    "    cudaHostAlloc(&h_data, size_bytes, cudaHostAllocDefault);\n",
    "    cudaMalloc(&d_data_in, size_bytes);\n",
    "    cudaMalloc(&d_data_out, size_bytes);\n",
    "\n",
    "    // Init with sequential numbers to make swizzle obvious\n",
    "    // value = row * N + col\n",
    "    for (int i = 0; i < M * N; i++) {\n",
    "        h_data[i].x = i; \n",
    "        h_data[i].y = i; \n",
    "        h_data[i].z = i; \n",
    "        h_data[i].w = i; \n",
    "    }\n",
    "\n",
    "    cudaMemcpy(d_data_in, h_data, size_bytes, cudaMemcpyHostToDevice);\n",
    "    cudaMemset(d_data_out, 0, size_bytes);\n",
    "\n",
    "    // -------------------------------------------------------------------------\n",
    "    // TMA SETUP - EXPLICITLY ENABLING SWIZZLE_128B\n",
    "    // -------------------------------------------------------------------------\n",
    "    auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled();\n",
    "    CUtensorMap tensor_map{};\n",
    "    \n",
    "    // Config: INT32 type, Rank 2\n",
    "    uint64_t tensor_shape[] = {N * INT4_COMPONENTS, M}; \n",
    "    uint64_t tensor_stride[] = {N * sizeof(int4)}; \n",
    "    \n",
    "    // BOX SHAPE: {32, 32} ints = {128 bytes, 32 rows}\n",
    "    // This satisfies the 128B constraint.\n",
    "    uint32_t smem_box[] = {BN * INT4_COMPONENTS, BM}; \n",
    "    uint32_t element_stride[] = {1, 1};\n",
    "\n",
    "    CUresult res = cuTensorMapEncodeTiled(\n",
    "        &tensor_map,\n",
    "        CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_INT32, \n",
    "        2,\n",
    "        d_data_in, \n",
    "        tensor_shape,       \n",
    "        tensor_stride,      \n",
    "        smem_box,     \n",
    "        element_stride,\n",
    "        CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "        CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B, // NOW VALID because box width is 128B\n",
    "        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "    );\n",
    "\n",
    "    if (res != CUDA_SUCCESS) { printf(\"TMA Encode failed %d\\n\", res); return 1; }\n",
    "\n",
    "    // Launch\n",
    "    // Grid X needs to cover N using blocks of BN\n",
    "    dim3 grid(N/BN, M/BM);\n",
    "    // Block dims must match our tile size\n",
    "    dim3 block(BN, BM);\n",
    "    \n",
    "    show_swizzle_kernel<<<grid, block>>>(d_data_out, tensor_map);\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "\n",
    "    // Read back\n",
    "    cudaMemcpy(h_data, d_data_out, size_bytes, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // -------------------------------------------------------------------------\n",
    "    // INSPECT RESULTS\n",
    "    // -------------------------------------------------------------------------\n",
    "    printf(\"Inspection of First Row (0 to 15):\\n\");\n",
    "    printf(\"Expected: 0, 1, 2, 3, 4, ...\\n\");\n",
    "    printf(\"Actual  : \");\n",
    "    \n",
    "    // We expect to see local permutations\n",
    "    for (int i = 0; i < 16; i++) {\n",
    "        printf(\"%d, \", h_data[i].x);\n",
    "    }\n",
    "    printf(\"...\\n\\n\");\n",
    "\n",
    "    // Check if swizzling happened\n",
    "    bool swizzled = false;\n",
    "    for (int i = 0; i < 16; i++) {\n",
    "        if (h_data[i].x != i) {\n",
    "            swizzled = true;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (swizzled) {\n",
    "        printf(\"SUCCESS: The data is scrambled! The TMA Swizzle pattern is visible.\\n\");\n",
    "    } else {\n",
    "        printf(\"FAILURE: The data looks linear. Swizzle 128B was not effective.\\n\");\n",
    "    }\n",
    "\n",
    "    cudaFree(d_data_in);\n",
    "    cudaFree(d_data_out);\n",
    "    cudaFreeHost(h_data);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774dc11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
