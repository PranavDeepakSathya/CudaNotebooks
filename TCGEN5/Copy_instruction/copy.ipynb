{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c49002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"/tmp/tmpwohn4s39\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter\n",
    "\n",
    "from nvcc4jupyter import set_defaults\n",
    "set_defaults(compiler_args='-arch=sm_100a -Xptxas=-v -O0 -I/workspace/cutlass/include')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf07b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpwohn4s39/4b175c7f-9c06-43d9-b968-704326085277/single_file.cu:10:40: warning: backslash and newline separated by space\n",
      "   10 | #define allocate_tmem(smem_dst, nCols) \\\n",
      "      |                                         \n",
      "/tmp/tmpwohn4s39/4b175c7f-9c06-43d9-b968-704326085277/single_file.cu:12:84: warning: backslash and newline separated by space\n",
      "   12 |     :                                                                              \\\n",
      "      |                                                                                     \n",
      "/tmp/tmpwohn4s39/4b175c7f-9c06-43d9-b968-704326085277/single_file.cu:16:39: warning: backslash and newline separated by space\n",
      "   16 | #define deallocate_tmem(taddr, nCols) \\\n",
      "      |                                        \n",
      "/tmp/tmpwohn4s39/4b175c7f-9c06-43d9-b968-704326085277/single_file.cu:18:78: warning: backslash and newline separated by space\n",
      "   18 |     :                                                                        \\\n",
      "      |                                                                               \n",
      "/tmp/tmpwohn4s39/4b175c7f-9c06-43d9-b968-704326085277/single_file.cu(27): warning #767-D: conversion from pointer to smaller integer\n",
      "    uint32_t smem_addr = (uint32_t) &A[0];\n",
      "                         ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/tmp/tmpwohn4s39/4b175c7f-9c06-43d9-b968-704326085277/single_file.cu(29): error: expected an expression\n",
      "    asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned{.shared::cta}.b32  [%0], %1\"); : : \"r\"(smem_addr), \"r\"(n_cols) : \"memory\");;\n",
      "                                                                                         ^\n",
      "\n",
      "/tmp/tmpwohn4s39/4b175c7f-9c06-43d9-b968-704326085277/single_file.cu(31): error: expected an expression\n",
      "    asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32    [%0], %1\"); : : \"r\"(taddr), \"r\"(n_cols) : \"memory\");;\n",
      "                                                                               ^\n",
      "\n",
      "2 errors detected in the compilation of \"/tmp/tmpwohn4s39/4b175c7f-9c06-43d9-b968-704326085277/single_file.cu\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdint.h>\n",
    "#include <cuda_bf16.h>\n",
    "\n",
    "#define allocate_tmem(smem_dst, nCols) \\ \n",
    "  asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned{.shared::cta}.b32  [%0], %1\"); \\\n",
    "    :                                                                              \\ \n",
    "    : \"r\"(smem_dst), \"r\"(nCols) \\\n",
    "    : \"memory\");\\\n",
    "               \n",
    "#define deallocate_tmem(taddr, nCols) \\ \n",
    "  asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32    [%0], %1\"); \\\n",
    "    :                                                                        \\ \n",
    "    : \"r\"(taddr), \"r\"(nCols) \\\n",
    "    : \"memory\");\\\n",
    "               \n",
    "constexpr int N_cols = 32; \n",
    "\n",
    "__global__ void alloc_dealloc_tmem()\n",
    "{\n",
    "  __shared__ uint32_t A[1]; \n",
    "  uint32_t smem_addr = (uint32_t) &A[0];\n",
    "  uint32_t n_cols = N_cols;\n",
    "  allocate_tmem(smem_addr, n_cols); \n",
    "  uint32_t taddr = A[0]; \n",
    "  deallocate_tmem(taddr, n_cols);\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "  alloc_dealloc_tmem<<<1,32>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b441bff",
   "metadata": {},
   "source": [
    "Ok this is so fucked and I am so pissed off right now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989d68d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 53\n",
      "00000000001001010000000000110101\n"
     ]
    }
   ],
   "source": [
    "l = 37 \n",
    "c = 53 \n",
    "def get_tmem_addr(lane, col): \n",
    "  return (c | (l<< 16))\n",
    "\n",
    "def get_2d_from_TMEM_addr(tmem_addr): \n",
    "  c_mask = (1 << 16) - 1 \n",
    "  col = tmem_addr & c_mask \n",
    "  row = tmem_addr >> 16\n",
    "  return row, col\n",
    "  \n",
    "l_new, c_new  = get_2d_from_TMEM_addr(get_tmem_addr(l,c))\n",
    "print(l_new, c_new)\n",
    "\n",
    "tmem_addr = get_tmem_addr(l,c)\n",
    "print(f\"{tmem_addr:0{32}b}\")\n",
    "\n",
    "def print_bin(n): \n",
    "  print(f\"{n:0{32}b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469104f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000000000110101\n"
     ]
    }
   ],
   "source": [
    "print_bin(tmem_addr % 2**16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049b539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000000000000000000100101\n"
     ]
    }
   ],
   "source": [
    "print_bin(tmem_addr // (2**16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebb7af02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpwohn4s39/b4d84de6-380d-44ca-8576-33a101d68ddd/single_file.cu(32): warning #767-D: conversion from pointer to smaller integer\n",
      "    uint32_t smem_addr = (uint32_t) &A[0];\n",
      "                         ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/tmp/tmpwohn4s39/b4d84de6-380d-44ca-8576-33a101d68ddd/single_file.cu(32): warning #767-D: conversion from pointer to smaller integer\n",
      "    uint32_t smem_addr = (uint32_t) &A[0];\n",
      "                         ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/tmp/tmpwohn4s39/b4d84de6-380d-44ca-8576-33a101d68ddd/single_file.cu(32): warning #767-D: conversion from pointer to smaller integer\n",
      "    uint32_t smem_addr = (uint32_t) &A[0];\n",
      "                         ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "ptxas /tmp/tmpxft_00000858_00000000-6_single_file.compute_100a.ptx, line 31; fatal   : Parsing error near 'ld': syntax error\n",
      "ptxas fatal   : Ptx assembly aborted due to errors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdint.h>\n",
    "#include <cuda_bf16.h>\n",
    "\n",
    "\n",
    "#define allocate_tmem(smem_dst, nCols) \\\n",
    "  asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32  [%0], %1\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"r\"(smem_dst), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "// FIX 1: Removed trailing whitespace and internal semicolons\n",
    "// FIX 3: Changed constraint for taddr (now taddr_ptr) from \"r\" to \"l\"\n",
    "#define deallocate_tmem(taddr_ptr, nCols) \\\n",
    "  asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32    %0, %1\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"r\"(taddr_ptr), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "constexpr int N_cols = 32;\n",
    "\n",
    "__global__ void alloc_dealloc_tmem()\n",
    "{\n",
    "  __shared__ uint32_t A[1];\n",
    "  \n",
    "\n",
    "  uint32_t smem_addr = (uint32_t) &A[0];\n",
    "  uint32_t n_cols = N_cols;\n",
    "  \n",
    "  allocate_tmem(smem_addr, n_cols);\n",
    "\n",
    "  uint32_t taddr = A[0];\n",
    "  \n",
    "\n",
    "  deallocate_tmem(taddr, n_cols);\n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "  alloc_dealloc_tmem<<<1,32>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  printf(\"Kernel executed successfully.\\n\"); // Added a print for confirmation\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707df238",
   "metadata": {},
   "source": [
    "KEEP THE BELOW CODE IT IS PRECIOUS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f1c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel executed successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdint.h>\n",
    "#include <cuda_bf16.h>\n",
    "\n",
    "#define allocate_tmem(smem_dst_ptr, nCols) \\\n",
    "  asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32  [%0], %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"l\"(smem_dst_ptr), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "\n",
    "#define deallocate_tmem(taddr_handle, nCols) \\\n",
    "  asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32    %0, %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"r\"(taddr_handle), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "constexpr int N_cols = 32;\n",
    "\n",
    "__global__ void alloc_dealloc_tmem()\n",
    "{\n",
    "  __shared__ uint32_t A[1]; \n",
    "  \n",
    "  uint64_t smem_addr = (uint64_t) &A[0];\n",
    "  uint32_t n_cols = N_cols;\n",
    "  \n",
    "\n",
    "  allocate_tmem(smem_addr, n_cols); \n",
    "  \n",
    "  uint32_t taddr = A[0]; \n",
    "  deallocate_tmem(taddr, n_cols);\n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "  alloc_dealloc_tmem<<<1,32>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  printf(\"Kernel executed successfully.\\n\");\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8882903",
   "metadata": {},
   "source": [
    "KEEP THE ABOVE CODE IT IS PRECIOUS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a96c621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel executed successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdint.h>\n",
    "#include <cuda_bf16.h>\n",
    "\n",
    "#define allocate_tmem(smem_dst_ptr, nCols) \\\n",
    "  asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32  [%0], %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"l\"(smem_dst_ptr), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "\n",
    "#define deallocate_tmem(taddr_handle, nCols) \\\n",
    "  asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32    %0, %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"r\"(taddr_handle), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "constexpr int N_cols = 32;\n",
    "\n",
    "__global__ void alloc_dealloc_tmem()\n",
    "{\n",
    "  int t = threadIdx.x; \n",
    "  \n",
    "  __shared__ uint32_t A[1]; \n",
    "  \n",
    "  uint64_t smem_addr = (uint64_t) &A[0];\n",
    "  uint32_t n_cols = N_cols;\n",
    "  \n",
    "\n",
    "  allocate_tmem(smem_addr, n_cols); \n",
    "  \n",
    "  uint32_t taddr = A[0]; \n",
    "  //#threads have arrived here\n",
    "  __syncthreads();\n",
    "  if (t == 0)\n",
    "  {\n",
    "    printf(\"This is the start of the address in TMEM, as stored in SMEM after allocation: %d \\n\", taddr);\n",
    "    \n",
    "  }\n",
    "  __syncthreads();\n",
    "  \n",
    "  deallocate_tmem(taddr, n_cols);\n",
    "\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "  alloc_dealloc_tmem<<<1,32>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  printf(\"Kernel executed successfully.\\n\");\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fb6afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel executed successfully.\n",
      "This is the start of the address in TMEM: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdint.h>\n",
    "#include <cuda_bf16.h>\n",
    "\n",
    "#define allocate_tmem(smem_dst_ptr, nCols) \\\n",
    "  asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32  [%0], %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"l\"(smem_dst_ptr), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "\n",
    "#define deallocate_tmem(taddr_handle, nCols) \\\n",
    "  asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32    %0, %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"r\"(taddr_handle), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "constexpr int N_cols = 32;\n",
    "\n",
    "__global__ void alloc_dealloc_tmem(uint32_t* d_debug_output) // <-- Pass in a global pointer\n",
    "{\n",
    "  int t = threadIdx.x; \n",
    "  \n",
    "  __shared__ uint32_t A[1]; \n",
    "  A[0] = 100;\n",
    "  \n",
    "  uint64_t smem_addr = (uint64_t) &A[0];\n",
    "  uint32_t n_cols = N_cols;\n",
    "  \n",
    "  allocate_tmem(smem_addr, n_cols); \n",
    "  \n",
    "  uint32_t taddr = A[0]; \n",
    "\n",
    "\n",
    "  deallocate_tmem(taddr, n_cols);\n",
    "  \n",
    "  if (t == 0)\n",
    "  {\n",
    "    *d_debug_output = A[0]; // Write the handle to global memory\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "\n",
    "  uint32_t* d_debug_val;\n",
    "  cudaMalloc(&d_debug_val, sizeof(uint32_t));\n",
    "\n",
    "  uint32_t h_debug_val = 12;\n",
    "\n",
    "  alloc_dealloc_tmem<<<1,32>>>(d_debug_val);\n",
    "  \n",
    "  cudaError_t err = cudaDeviceSynchronize();\n",
    "  if (err != cudaSuccess) {\n",
    "      printf(\"Kernel failed: %s\\n\", cudaGetErrorString(err));\n",
    "  }\n",
    "\n",
    "  cudaMemcpy(&h_debug_val, d_debug_val, sizeof(uint32_t), cudaMemcpyDeviceToHost);\n",
    "\n",
    "  printf(\"Kernel executed successfully.\\n\");\n",
    "  printf(\"This is the start of the address in TMEM: %u \\n\", h_debug_val);\n",
    "  cudaFree(d_debug_val);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d740c",
   "metadata": {},
   "source": [
    "if every warp can allocate 32 columns of tmem and store its start address, then we can deploy 2 warps, and make them \n",
    "allocate different bits of tmem and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69778f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host array before kernel (random values):\n",
      "h_debug_val[0] = 1391302528\n",
      "h_debug_val[1] = 1093967198\n",
      "\n",
      "Kernel executed successfully.\n",
      "TMEM addresses from device:\n",
      "Warp 0 TMEM address: 0 \n",
      "Warp 1 TMEM address: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdint.h>\n",
    "#include <cuda_bf16.h>\n",
    "#include <time.h>\n",
    "\n",
    "\n",
    "\n",
    "#define allocate_tmem(smem_dst_ptr, nCols) \\\n",
    "  asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32  [%0], %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"l\"(smem_dst_ptr), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "\n",
    "#define deallocate_tmem(taddr_handle, nCols) \\\n",
    "  asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32    %0, %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"r\"(taddr_handle), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "constexpr int N_cols = 32;\n",
    "constexpr int N_ctas = 1; \n",
    "constexpr int N_warps = 2; \n",
    "constexpr int N_threads_per_block = N_warps*32;\n",
    "\n",
    "__global__ void alloc_dealloc_tmem(uint32_t* d_debug_output)\n",
    "{\n",
    "  int t = threadIdx.x; \n",
    "  int warp_id = t / 32;\n",
    "  int lane_id = t % 32;\n",
    "  \n",
    "  __shared__ uint32_t A[N_warps]; \n",
    "  \n",
    "  uint64_t smem_addr = (uint64_t) &A[warp_id];\n",
    "  uint32_t n_cols = N_cols;\n",
    "  \n",
    "  allocate_tmem(smem_addr, n_cols); \n",
    "  \n",
    "  uint32_t taddr = A[warp_id]; \n",
    "\n",
    "  deallocate_tmem(taddr, n_cols);\n",
    "  \n",
    "  if (lane_id == 0)\n",
    "  {\n",
    "    d_debug_output[warp_id] = A[warp_id];\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "\n",
    "  uint32_t* d_debug_val;\n",
    "  uint32_t* h_debug_val;\n",
    "  size_t size = N_warps * sizeof(uint32_t);\n",
    "  \n",
    "  cudaMalloc(&d_debug_val, size);\n",
    "  h_debug_val = (uint32_t*)malloc(size);\n",
    "\n",
    "  srand(time(NULL));\n",
    "  printf(\"Host array before kernel (random values):\\n\");\n",
    "  for (int i = 0; i < N_warps; i++) {\n",
    "    h_debug_val[i] = rand();\n",
    "    printf(\"h_debug_val[%d] = %u\\n\", i, h_debug_val[i]);\n",
    "  }\n",
    "\n",
    "  alloc_dealloc_tmem<<<N_ctas,N_threads_per_block>>>(d_debug_val);\n",
    "  \n",
    "  cudaError_t err = cudaDeviceSynchronize();\n",
    "  if (err != cudaSuccess) {\n",
    "      printf(\"Kernel failed: %s\\n\", cudaGetErrorString(err));\n",
    "  }\n",
    "\n",
    "  cudaMemcpy(h_debug_val, d_debug_val, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "  printf(\"\\nKernel executed successfully.\\n\");\n",
    "  printf(\"TMEM addresses from device:\\n\");\n",
    "  for (int i = 0; i < N_warps; i++) {\n",
    "    printf(\"Warp %d TMEM address: %u \\n\", i, h_debug_val[i]);\n",
    "  }\n",
    "\n",
    "  cudaFree(d_debug_val);\n",
    "  free(h_debug_val);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "115550e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host array before kernel (random values):\n",
      "h_debug_val[0] = 771223414\n",
      "h_debug_val[1] = 181268272\n",
      "\n",
      "Kernel executed successfully.\n",
      "TMEM addresses from device:\n",
      "CTA 0 TMEM address: 0 \n",
      "CTA 1 TMEM address: 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdint.h>\n",
    "#include <cuda_bf16.h>\n",
    "#include <time.h>\n",
    "\n",
    "\n",
    "\n",
    "#define allocate_tmem(smem_dst_ptr, nCols) \\\n",
    "  asm volatile(\"tcgen05.alloc.cta_group::2.sync.aligned.shared::cta.b32  [%0], %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"l\"(smem_dst_ptr), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "\n",
    "#define deallocate_tmem(taddr_handle, nCols) \\\n",
    "  asm volatile(\"tcgen05.dealloc.cta_group::2.sync.aligned.b32    %0, %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"r\"(taddr_handle), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "constexpr int N_cols = 32;\n",
    "constexpr int N_ctas = 2; \n",
    "constexpr int N_warps = 1; \n",
    "constexpr int N_threads_per_block = N_warps*32;\n",
    "\n",
    "__global__ void alloc_dealloc_tmem(uint32_t* d_debug_output)\n",
    "{\n",
    "  \n",
    "  int b = blockIdx.x; \n",
    "  \n",
    "  __shared__ uint32_t A[1]; \n",
    "  \n",
    "  uint64_t smem_addr = (uint64_t) &A[0];\n",
    "  uint32_t n_cols = N_cols;\n",
    "  \n",
    "  allocate_tmem(smem_addr, n_cols); \n",
    "  \n",
    "  uint32_t taddr = A[0]; \n",
    "\n",
    "  deallocate_tmem(taddr, n_cols);\n",
    "  \n",
    "\n",
    "  d_debug_output[b] = A[0];\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "\n",
    "  uint32_t* d_debug_val;\n",
    "  uint32_t* h_debug_val;\n",
    "  size_t size = N_ctas * sizeof(uint32_t);\n",
    "  \n",
    "  cudaMalloc(&d_debug_val, size);\n",
    "  h_debug_val = (uint32_t*)malloc(size);\n",
    "\n",
    "  srand(time(NULL));\n",
    "  printf(\"Host array before kernel (random values):\\n\");\n",
    "  for (int i = 0; i < N_ctas; i++) {\n",
    "    h_debug_val[i] = rand();\n",
    "    printf(\"h_debug_val[%d] = %u\\n\", i, h_debug_val[i]);\n",
    "  }\n",
    "\n",
    "  alloc_dealloc_tmem<<<N_ctas,N_threads_per_block>>>(d_debug_val);\n",
    "  \n",
    "  cudaError_t err = cudaDeviceSynchronize();\n",
    "  if (err != cudaSuccess) {\n",
    "      printf(\"Kernel failed: %s\\n\", cudaGetErrorString(err));\n",
    "  }\n",
    "\n",
    "  cudaMemcpy(h_debug_val, d_debug_val, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "  printf(\"\\nKernel executed successfully.\\n\");\n",
    "  printf(\"TMEM addresses from device:\\n\");\n",
    "  for (int i = 0; i < N_ctas; i++) {\n",
    "    printf(\"CTA %d TMEM address: %u \\n\", i, h_debug_val[i]);\n",
    "  }\n",
    "\n",
    "  cudaFree(d_debug_val);\n",
    "  free(h_debug_val);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e523c5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host array before kernel (random values):\n",
      "h_debug_val[0] = 934915272\n",
      "h_debug_val[1] = 634184069\n",
      "h_debug_val[2] = 1806837172\n",
      "h_debug_val[3] = 1144539932\n",
      "\n",
      "Kernel executed successfully.\n",
      "TMEM addresses from device:\n",
      "CTA 0 TMEM address: 934915272 \n",
      "CTA 1 TMEM address: 634184069 \n",
      "CTA 2 TMEM address: 1806837172 \n",
      "CTA 3 TMEM address: 1144539932 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_fp16.h>\n",
    "#include <stdint.h>\n",
    "#include <cuda_bf16.h>\n",
    "#include <time.h>\n",
    "\n",
    "\n",
    "\n",
    "#define allocate_tmem(smem_dst_ptr, nCols) \\\n",
    "  asm volatile(\"tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32  [%0], %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"l\"(smem_dst_ptr), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "\n",
    "#define deallocate_tmem(taddr_handle, nCols) \\\n",
    "  asm volatile(\"tcgen05.dealloc.cta_group::1.sync.aligned.b32    %0, %1;\" \\\n",
    "    : /* no outputs */ \\\n",
    "    : \"r\"(taddr_handle), \"r\"(nCols) \\\n",
    "    : \"memory\")\n",
    "\n",
    "constexpr int N_cols = 32;\n",
    "constexpr int N_ctas = 1; \n",
    "constexpr int N_warps = 1; \n",
    "constexpr int N_threads_per_block = N_warps*32;\n",
    "  constexpr int N_debug_vals = 4;\n",
    "\n",
    "__global__ void alloc_dealloc_tmem(uint32_t* d_debug_output)\n",
    "{\n",
    "  \n",
    "  int b = blockIdx.x; \n",
    "  int t = threadIdx.x;\n",
    "  \n",
    "  __shared__ uint32_t A[2]; \n",
    "  \n",
    "  uint64_t smem_addr_0 = (uint64_t) &A[0];\n",
    "  uint64_t smem_addr_1 = (uint64_t) &A[1];\n",
    "  uint32_t n_cols = N_cols;\n",
    "  \n",
    "  \n",
    "  allocate_tmem(smem_addr_0, n_cols); \n",
    "  \n",
    "  allocate_tmem(smem_addr_1, n_cols);\n",
    "  __syncthreads();\n",
    "  \n",
    "    if (t == 0)\n",
    "  {\n",
    "    d_debug_output[2*b + 0] = A[0]; \n",
    "    d_debug_output[2*b + 1] = A[1]; \n",
    "  }\n",
    "  __syncthreads();\n",
    "  uint32_t taddr_0 = A[0]; \n",
    "  uint32_t taddr_1 = A[1];\n",
    "\n",
    "  deallocate_tmem(taddr_0, n_cols);\n",
    "  deallocate_tmem(taddr_1, n_cols);\n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "int main()\n",
    "{\n",
    "\n",
    "\n",
    "  uint32_t* d_debug_val;\n",
    "  uint32_t* h_debug_val;\n",
    "  size_t size = N_debug_vals * sizeof(uint32_t);\n",
    "  \n",
    "  cudaMalloc(&d_debug_val, size);\n",
    "  h_debug_val = (uint32_t*)malloc(size);\n",
    "\n",
    "  srand(time(NULL));\n",
    "  printf(\"Host array before kernel (random values):\\n\");\n",
    "  for (int i = 0; i < N_debug_vals; i++) {\n",
    "    h_debug_val[i] = rand();\n",
    "    printf(\"h_debug_val[%d] = %u\\n\", i, h_debug_val[i]);\n",
    "  }\n",
    "  cudaMemcpy(d_debug_val, h_debug_val, size, cudaMemcpyHostToDevice);\n",
    "  alloc_dealloc_tmem<<<N_ctas,N_threads_per_block>>>(d_debug_val);\n",
    "  \n",
    "  cudaError_t err = cudaDeviceSynchronize();\n",
    "  if (err != cudaSuccess) {\n",
    "      printf(\"Kernel failed: %s\\n\", cudaGetErrorString(err));\n",
    "  }\n",
    "\n",
    "  cudaMemcpy(h_debug_val, d_debug_val, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "  printf(\"\\nKernel executed successfully.\\n\");\n",
    "  printf(\"TMEM addresses from device:\\n\");\n",
    "  for (int i = 0; i < N_debug_vals; i++) {\n",
    "    printf(\"CTA %d TMEM address: %u \\n\", i, h_debug_val[i]);\n",
    "  }\n",
    "\n",
    "  cudaFree(d_debug_val);\n",
    "  free(h_debug_val);\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190411d0",
   "metadata": {},
   "source": [
    "Ah now we see the issue, the allocation isn't even happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3904c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
