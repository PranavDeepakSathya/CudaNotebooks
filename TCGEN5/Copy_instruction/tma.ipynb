{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c183e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"/tmp/tmpmmvbn4mi\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter\n",
    "\n",
    "from nvcc4jupyter import set_defaults\n",
    "set_defaults(compiler_args='-arch=sm_120 -Xptxas=-v -o out.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1f81ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D[0]: 1 D[1]: 1 D[2]: 1 D[3]: 1 D[4]: 1 D[5]: 1 D[6]: 1 D[7]: 1 D[8]: 1 D[9]: 1 D[10]: 1 D[11]: 1 D[12]: 1 D[13]: 1 D[14]: 1 D[15]: 1 D[16]: 1 D[17]: 1 D[18]: 1 D[19]: 1 D[20]: 1 D[21]: 1 D[22]: 1 D[23]: 1 D[24]: 1 D[25]: 1 D[26]: 1 D[27]: 1 D[28]: 1 D[29]: 1 D[30]: 1 D[31]: 1 D[32]: 1 D[33]: 1 D[34]: 1 D[35]: 1 D[36]: 1 D[37]: 1 D[38]: 1 D[39]: 1 D[40]: 1 D[41]: 1 D[42]: 1 D[43]: 1 D[44]: 1 D[45]: 1 D[46]: 1 D[47]: 1 D[48]: 1 D[49]: 1 D[50]: 1 D[51]: 1 D[52]: 1 D[53]: 1 D[54]: 1 D[55]: 1 D[56]: 1 D[57]: 1 D[58]: 1 D[59]: 1 D[60]: 1 D[61]: 1 D[62]: 1 D[63]: 1 D[64]: 1 D[65]: 1 D[66]: 1 D[67]: 1 D[68]: 1 D[69]: 1 D[70]: 1 D[71]: 1 D[72]: 1 D[73]: 1 D[74]: 1 D[75]: 1 D[76]: 1 D[77]: 1 D[78]: 1 D[79]: 1 D[80]: 1 D[81]: 1 D[82]: 1 D[83]: 1 D[84]: 1 D[85]: 1 D[86]: 1 D[87]: 1 D[88]: 1 D[89]: 1 D[90]: 1 D[91]: 1 D[92]: 1 D[93]: 1 D[94]: 1 D[95]: 1 D[96]: 1 D[97]: 1 D[98]: 1 D[99]: 1 \n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include<stdio.h>\n",
    "#include<stdlib.h>\n",
    "#include<cuda_runtime.h>\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace ptx = cuda::ptx;\n",
    "\n",
    "static constexpr size_t buf_len = 1024;\n",
    "static constexpr int N_blocks = 4;\n",
    "static constexpr size_t data_len = N_blocks * buf_len;\n",
    "static constexpr int N_threads_per_block = 64;\n",
    "__global__ void add_one_kernel(int* data, size_t offset)\n",
    "{\n",
    "  //# Shared memory buffer. The destination shared memory buffer of\n",
    "  //# a bulk operations should be 16 byte aligned.\n",
    "  __shared__ alignas(16) int smem_data[buf_len];\n",
    "\n",
    "  //# 1. a) Initialize shared memory barrier with the number of threads participating in the barrier.\n",
    "  //#3    b) Make initialized barrier visible in async proxy.\n",
    "  #pragma nv_diag_suppress static_var_with_dynamic_init\n",
    "  __shared__ barrier bar;\n",
    "  if (threadIdx.x == 0) { \n",
    "    init(&bar, blockDim.x);                      // a)\n",
    "    ptx::fence_proxy_async(ptx::space_shared);   // b)\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  //# 2. Initiate TMA transfer to copy global to shared memory.\n",
    "  if (threadIdx.x == 0) {\n",
    "    //# 3a. cuda::memcpy_async arrives on the barrier and communicates\n",
    "    //#     how many bytes are expected to come in (the transaction count)\n",
    "    cuda::memcpy_async(\n",
    "        smem_data, \n",
    "        data + offset, \n",
    "        cuda::aligned_size_t<16>(sizeof(smem_data)),\n",
    "        bar\n",
    "    );\n",
    "  }\n",
    "  //# 3b. All threads arrive on the barrier\n",
    "  barrier::arrival_token token = bar.arrive();\n",
    "  \n",
    "  //# 3c. Wait for the data to have arrived.\n",
    "  bar.wait(std::move(token));\n",
    "\n",
    "  //# 4. Compute saxpy and write back to shared memory\n",
    "  for (int i = threadIdx.x; i < buf_len; i += blockDim.x) {\n",
    "    smem_data[i] += 1;\n",
    "  }\n",
    "\n",
    "  //# 5. Wait for shared memory writes to be visible to TMA engine.\n",
    "  ptx::fence_proxy_async(ptx::space_shared);   // b)\n",
    "  __syncthreads();\n",
    "  //# After syncthreads, writes by all threads are visible to TMA engine.\n",
    "\n",
    "  // #6. Initiate TMA transfer to copy shared memory to global memory\n",
    "  if (threadIdx.x == 0) {\n",
    "    ptx::cp_async_bulk(\n",
    "        ptx::space_global,\n",
    "        ptx::space_shared,\n",
    "        data + offset, smem_data, sizeof(smem_data));\n",
    "    // 7. Wait for TMA transfer to have finished reading shared memory.\n",
    "    // Create a \"bulk async-group\" out of the previous bulk copy operation.\n",
    "    ptx::cp_async_bulk_commit_group();\n",
    "    // Wait for the group to have completed reading from shared memory.\n",
    "    ptx::cp_async_bulk_wait_group_read(ptx::n32_t<0>());\n",
    "  }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  int*data_h; \n",
    "  cudaHostAlloc(&data_h, data_len, cudaHostAllocDefault); \n",
    "  int*data_d; \n",
    "  size_t offset = 0;\n",
    "  cudaMalloc(&data_d, data_len); \n",
    "  cudaMemcpy(data_d, data_h, data_len, cudaMemcpyHostToDevice); \n",
    "  add_one_kernel<<<N_blocks, N_threads_per_block>>>(data_d, offset);\n",
    "  cudaDeviceSynchronize(); \n",
    "  cudaMemcpy(data_h, data_d, data_len, cudaMemcpyDeviceToHost); \n",
    "  for (int i = 0; i < 100; i ++)\n",
    "  {\n",
    "    printf(\"D[%d]: %d \", i, data_h[i]);\n",
    "  }\n",
    "  cudaFreeHost(data_h); \n",
    "  cudaFree(data_d);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "203456fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A[0]: 0.000000 , A[1]: 0.000000 , A[2]: 0.000000 , A[3]: 0.000000 , A[4]: 0.000000 , A[5]: 0.000000 , A[6]: 0.000000 , A[7]: 0.000000 , A[8]: 0.000000 , A[9]: 0.000000 , A[10]: 0.000000 , A[11]: 0.000000 , A[12]: 0.000000 , A[13]: 0.000000 , A[14]: 0.000000 , A[15]: 0.000000 , A[16]: 0.000000 , A[17]: 0.000000 , A[18]: 0.000000 , A[19]: 0.000000 , A[20]: 0.000000 , A[21]: 0.000000 , A[22]: 0.000000 , A[23]: 0.000000 , A[24]: 0.000000 , A[25]: 0.000000 , A[26]: 0.000000 , A[27]: 0.000000 , A[28]: 0.000000 , A[29]: 0.000000 , A[30]: 0.000000 , A[31]: 0.000000 , \n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include<stdio.h>\n",
    "#include<stdlib.h>\n",
    "#include<cuda_runtime.h>\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "#include<cuda.h>\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace ptx = cuda::ptx;\n",
    "namespace cde = cuda::device::experimental;\n",
    "#ifndef CUDA_UTILS_H\n",
    "#define CUDA_UTILS_H\n",
    "\n",
    "\n",
    "\n",
    "/**\n",
    " * @brief Macro to wrap CUDA API calls.\n",
    " * * Usage: CUDA_CHECK(cudaMalloc(&d_a, size));\n",
    " */\n",
    "#define CUDA_CHECK(call) \\\n",
    "    { \\\n",
    "        gpuAssert((call), __FILE__, __LINE__); \\\n",
    "    }\n",
    "\n",
    "/**\n",
    " * @brief Internal function to handle the error logic.\n",
    " * * @param code The error code returned by the CUDA function.\n",
    " * @param file The file name where the error occurred (provided by __FILE__).\n",
    " * @param line The line number where the error occurred (provided by __LINE__).\n",
    " * @param abort Whether to terminate the application on error (default: true).\n",
    " */\n",
    "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true)\n",
    "{\n",
    "    if (code != cudaSuccess) \n",
    "    {\n",
    "        // Print the error name (e.g., cudaErrorMemoryAllocation)\n",
    "        // and the description (e.g., \"out of memory\")\n",
    "        fprintf(stderr, \"CUDA_CHECK Error: %s %s\\n\", \n",
    "                cudaGetErrorName(code), \n",
    "                cudaGetErrorString(code));\n",
    "        \n",
    "        fprintf(stderr, \"  File: %s\\n\", file);\n",
    "        fprintf(stderr, \"  Line: %d\\n\", line);\n",
    "\n",
    "        if (abort) \n",
    "        {\n",
    "            // Optional: Reset device to flush profiling data before exit\n",
    "            cudaDeviceReset();\n",
    "            exit(code);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Macro to check for errors after a kernel launch.\n",
    " * * Kernel launches are asynchronous. This macro checks:\n",
    " * 1. If the launch itself failed (invalid configuration).\n",
    " * 2. Synchronizes the device to check for execution errors (bus errors, segfaults).\n",
    " * * WARNING: Using this slows down code significantly because of the synchronization.\n",
    " * Use only for debugging.\n",
    " */\n",
    "#define CUDA_CHECK_KERNEL() \\\n",
    "    { \\\n",
    "        cudaError_t err = cudaGetLastError(); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"Kernel Launch Error (Sync): %s\\n\", cudaGetErrorString(err)); \\\n",
    "            gpuAssert(err, __FILE__, __LINE__); \\\n",
    "        } \\\n",
    "        err = cudaDeviceSynchronize(); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"Kernel Execution Error (Async): %s\\n\", cudaGetErrorString(err)); \\\n",
    "            gpuAssert(err, __FILE__, __LINE__); \\\n",
    "        } \\\n",
    "    }\n",
    "\n",
    "#endif // CUDA_UTILS_H\n",
    "\n",
    "#include <cudaTypedefs.h> // PFN_cuTensorMapEncodeTiled, CUtensorMap\n",
    "\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "  // Get pointer to cuTensorMapEncodeTiled\n",
    "  cudaDriverEntryPointQueryResult driver_status;\n",
    "  void* cuTensorMapEncodeTiled_ptr = nullptr;\n",
    "  CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &cuTensorMapEncodeTiled_ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "  assert(driver_status == cudaDriverEntryPointSuccess);\n",
    "\n",
    "  return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(cuTensorMapEncodeTiled_ptr);\n",
    "}\n",
    "constexpr uint32_t N = 32; \n",
    "constexpr uint32_t BN = 32;\n",
    "constexpr uint32_t rank = 2; \n",
    "constexpr uint32_t GMEM_WIDTH = N; \n",
    "constexpr uint32_t GMEM_HEIGHT = N; \n",
    "constexpr uint32_t SMEM_WIDTH = BN; \n",
    "constexpr uint32_t SMEM_HEIGHT = BN;\n",
    "uint64_t size[rank] = {GMEM_WIDTH, GMEM_HEIGHT}; //#the faster dimension is always left first. unit is number of elements\n",
    "uint64_t stride[rank-1] =  {GMEM_WIDTH*sizeof(float)}; //#the number of bytes to step from one row to the next, we are making a row major one here\n",
    "uint32_t box_size[rank] = {SMEM_WIDTH, SMEM_HEIGHT}; //# I wonder if the reduced stride is taken automatically, but \n",
    "//# our boys at NVDA say that things are assumed LINEAR in shared memory so I guess they don't give a fuck, but at this point \n",
    "//# the layout is already decided.\n",
    "uint32_t elem_stride[rank] = {1,1}; //# THIS VIDEO IS ALREADY TOO LONG FOR INSTAGRAM REEEEEELLLL FFS \n",
    "//# indeed, the element_strides are something along the lines of if each element is a row major 2 vector then {1,2} would be the elem stride\n",
    "//# but we have scalar elements so we guuci.\n",
    "\n",
    "__global__ void tma_kernel(float*A, const __grid_constant__ CUtensorMap tensor_map)\n",
    "{\n",
    "    __shared__ alignas(128) float smem_buffer[SMEM_HEIGHT][SMEM_WIDTH];\n",
    "      #pragma nv_diag_suppress static_var_with_dynamic_init\n",
    "    __shared__ barrier bar;\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        // #Initialize barrier. All `blockDim.x` threads in block participate.\n",
    "        init(&bar, blockDim.x);\n",
    "        // #Make initialized barrier visible in async proxy.\n",
    "        cde::fence_proxy_async_shared_cta();\n",
    "    }\n",
    "    // #Syncthreads so initialized barrier is visible to all threads.\n",
    "    __syncthreads();\n",
    "    barrier::arrival_token token;\n",
    "    if (threadIdx.x == 0) {\n",
    "        // #Initiate bulk tensor copy.\n",
    "        cde::cp_async_bulk_tensor_2d_global_to_shared(&smem_buffer, &tensor_map, 0, 0, bar);\n",
    "        // #Arrive on the barrier and tell how many bytes are expected to come in.\n",
    "        token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(smem_buffer));\n",
    "    } else {\n",
    "        //# Other threads just arrive.\n",
    "        token = bar.arrive();\n",
    "    }\n",
    "    // #Wait for the data to have arrived.\n",
    "    bar.wait(std::move(token));\n",
    "      smem_buffer[0][threadIdx.x] += (float) 32.0;\n",
    "\n",
    "  // #Wait for shared memory writes to be visible to TMA engine.\n",
    "  cde::fence_proxy_async_shared_cta();\n",
    "  __syncthreads();\n",
    "  // #After syncthreads, writes by all threads are visible to TMA engine.\n",
    "\n",
    "  // #Initiate TMA transfer to copy shared memory to global memory\n",
    "  if (threadIdx.x == 0) {\n",
    "    cde::cp_async_bulk_tensor_2d_shared_to_global(&tensor_map, 0, 0, &smem_buffer);\n",
    "    //# Wait for TMA transfer to have finished reading shared memory.\n",
    "    // #Create a \"bulk async-group\" out of the previous bulk copy operation.\n",
    "    cde::cp_async_bulk_commit_group();\n",
    "    // #Wait for the group to have completed reading from shared memory.\n",
    "    cde::cp_async_bulk_wait_group_read<0>();\n",
    "  }\n",
    "\n",
    "  //# Destroy barrier. This invalidates the memory region of the barrier. If\n",
    "  //# further computations were to take place in the kernel, this allows the\n",
    "  //# memory location of the shared memory barrier to be reused.\n",
    "  if (threadIdx.x == 0) {\n",
    "    (&bar)->~barrier();\n",
    "  }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    float* A_h, *A_d; \n",
    "    size_t size_tensor = N*N*sizeof(float);\n",
    "    cudaHostAlloc(&A_h, size_tensor, cudaHostAllocDefault); \n",
    "    cudaMalloc(&A_d, size_tensor); \n",
    "    void *tensor_ptr =  &A_d; \n",
    "    cudaMemcpy(A_d, A_h, size_tensor, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    CUtensorMap tensor_map{};\n",
    "    auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled(); //#function pointer to the tensor map creation api\n",
    "    CUresult res = cuTensorMapEncodeTiled(\n",
    "    &tensor_map,                //# CUtensorMap *tensorMap,\n",
    "    CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_INT32,\n",
    "    rank,                       //# cuuint32_t tensorRank,\n",
    "    tensor_ptr,                 //# void *globalAddress,\n",
    "    size,                       //# const cuuint64_t *globalDim,\n",
    "    stride,                     //# const cuuint64_t *globalStrides,\n",
    "    box_size,                   //# const cuuint32_t *boxDim,\n",
    "    elem_stride,                //# const cuuint32_t *elementStrides,\n",
    "    //# Interleave patterns can be used to accelerate loading of values that\n",
    "    //# are less than 4 bytes long.\n",
    "    CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "    //# Swizzling can be used to avoid shared memory bank conflicts.\n",
    "    CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE,\n",
    "    //# L2 Promotion can be used to widen the effect of a cache-policy to a wider\n",
    "    //# set of L2 cache lines.\n",
    "    CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "    //# Any element that is outside of bounds will be set to zero by the TMA transfer.\n",
    "    CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "    \n",
    "    \n",
    ");\n",
    "    tma_kernel<<<1,32>>>(A_d, tensor_map);\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaMemcpy(A_h, A_d, size_tensor, cudaMemcpyDeviceToHost);\n",
    "    for (int i = 0; i < 32; i++)\n",
    "    {\n",
    "        printf(\"A[%d]: %f , \", i, A_h[i]);\n",
    "    }\n",
    "    cudaFreeHost(A_h); \n",
    "    cudaFree(A_d);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3d86ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 32 elements (Row 0):\n",
      "32.0 33.0 34.0 35.0 36.0 37.0 38.0 39.0 40.0 41.0 42.0 43.0 44.0 45.0 46.0 47.0 48.0 49.0 50.0 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0 60.0 61.0 62.0 63.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <assert.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cudaTypedefs.h> \n",
    "\n",
    "// Modern C++ CUDA headers for TMA\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "\n",
    "// Namespaces for cleaner code\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace cde = cuda::device::experimental;\n",
    "\n",
    "// --- Helper Macros ---\n",
    "#define CUDA_CHECK(call) \\\n",
    "    { \\\n",
    "        cudaError_t err = (call); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n",
    "                    cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "            exit(err); \\\n",
    "        } \\\n",
    "    }\n",
    "\n",
    "// --- Driver API Helper ---\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "    cudaDriverEntryPointQueryResult driver_status;\n",
    "    void* ptr = nullptr;\n",
    "    CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "    return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(ptr);\n",
    "}\n",
    "\n",
    "// --- Constants ---\n",
    "constexpr uint32_t N = 32; \n",
    "constexpr uint32_t BN = 32;\n",
    "constexpr uint32_t rank = 2; \n",
    "\n",
    "// --- Kernel ---\n",
    "__global__ void tma_kernel(float* A, const __grid_constant__ CUtensorMap tensor_map)\n",
    "{\n",
    "    // 1. Shared Memory Buffer (Aligned for TMA)\n",
    "    __shared__ alignas(128) float smem_buffer[BN][BN];\n",
    "    \n",
    "    // 2. MBarrier for synchronization\n",
    "    #pragma nv_diag_suppress static_var_with_dynamic_init\n",
    "    __shared__ barrier bar;\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        init(&bar, blockDim.x);\n",
    "        cde::fence_proxy_async_shared_cta();\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    // 3. TMA Load (Global -> Shared)\n",
    "    barrier::arrival_token token;\n",
    "    if (threadIdx.x == 0) {\n",
    "        cde::cp_async_bulk_tensor_2d_global_to_shared(&smem_buffer, &tensor_map, 0, 0, bar);\n",
    "        token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(smem_buffer));\n",
    "    } else {\n",
    "        token = bar.arrive();\n",
    "    }\n",
    "    bar.wait(std::move(token));\n",
    "\n",
    "    // 4. Modify Data\n",
    "    // Each thread modifies one element in the first row for demonstration\n",
    "    if (threadIdx.x < BN) {\n",
    "        smem_buffer[0][threadIdx.x] += 32.0f;\n",
    "    }\n",
    "\n",
    "    // 5. Fence to ensure writes are visible to TMA engine\n",
    "    cde::fence_proxy_async_shared_cta();\n",
    "    __syncthreads();\n",
    "\n",
    "    // 6. TMA Store (Shared -> Global)\n",
    "    if (threadIdx.x == 0) {\n",
    "        cde::cp_async_bulk_tensor_2d_shared_to_global(&tensor_map, 0, 0, &smem_buffer);\n",
    "        cde::cp_async_bulk_commit_group();\n",
    "        cde::cp_async_bulk_wait_group_read<0>();\n",
    "        // Note: We do not need wait_group_read here, as that is for loads.\n",
    "        // The store is committed and will complete asynchronously.\n",
    "    }\n",
    "\n",
    "    // 7. Cleanup Barrier\n",
    "    if (threadIdx.x == 0) {\n",
    "        (&bar)->~barrier();\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    float *A_h, *A_d; \n",
    "    size_t size_tensor = N * N * sizeof(float);\n",
    "    \n",
    "    // Allocation\n",
    "    CUDA_CHECK(cudaHostAlloc(&A_h, size_tensor, cudaHostAllocDefault)); \n",
    "    CUDA_CHECK(cudaMalloc(&A_d, size_tensor)); \n",
    "\n",
    "    // Initialize Host Data\n",
    "    for(int i=0; i<N*N; i++) A_h[i] = (float)i;\n",
    "    CUDA_CHECK(cudaMemcpy(A_d, A_h, size_tensor, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Tensor Map Configuration\n",
    "    uint64_t size[rank] = {N, N}; // Elements\n",
    "    uint64_t stride[rank-1] = {N * sizeof(float)}; // Bytes\n",
    "    uint32_t box_size[rank] = {BN, BN}; \n",
    "    uint32_t elem_stride[rank] = {1, 1};\n",
    "\n",
    "    // --- FIX: Pass the device pointer directly, cast to void* ---\n",
    "    void *tensor_ptr = (void*)A_d; \n",
    "\n",
    "    CUtensorMap tensor_map{};\n",
    "    auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled();\n",
    "\n",
    "    CUresult res = cuTensorMapEncodeTiled(\n",
    "        &tensor_map,\n",
    "        // Improved: Use FLOAT32 to match your data\n",
    "        CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT32, \n",
    "        rank,\n",
    "        tensor_ptr, // <--- The Fix\n",
    "        size,\n",
    "        stride,\n",
    "        box_size,\n",
    "        elem_stride,\n",
    "        CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "        CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE,\n",
    "        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "    );\n",
    "\n",
    "    if (res != CUDA_SUCCESS) {\n",
    "        printf(\"Tensor Map Encode Failed!\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Launch\n",
    "    tma_kernel<<<1, 32>>>(A_d, tensor_map);\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    \n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "\n",
    "    // Verify\n",
    "    CUDA_CHECK(cudaMemcpy(A_h, A_d, size_tensor, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"First 32 elements (Row 0):\\n\");\n",
    "    for (int i = 0; i < 32; i++) {\n",
    "        // Expected: Original Value (i) + 32.0\n",
    "        printf(\"%.1f \", A_h[i]);\n",
    "        \n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    cudaFreeHost(A_h); \n",
    "    cudaFree(A_d);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf6cf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL elements :\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \n",
      "36 37 38 39 32 33 34 35 44 45 46 47 40 41 42 43 52 53 54 55 48 49 50 51 60 61 62 63 56 57 58 59 \n",
      "72 73 74 75 76 77 78 79 64 65 66 67 68 69 70 71 88 89 90 91 92 93 94 95 80 81 82 83 84 85 86 87 \n",
      "108 109 110 111 104 105 106 107 100 101 102 103 96 97 98 99 124 125 126 127 120 121 122 123 116 117 118 119 112 113 114 115 \n",
      "144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 \n",
      "180 181 182 183 176 177 178 179 188 189 190 191 184 185 186 187 164 165 166 167 160 161 162 163 172 173 174 175 168 169 170 171 \n",
      "216 217 218 219 220 221 222 223 208 209 210 211 212 213 214 215 200 201 202 203 204 205 206 207 192 193 194 195 196 197 198 199 \n",
      "252 253 254 255 248 249 250 251 244 245 246 247 240 241 242 243 236 237 238 239 232 233 234 235 228 229 230 231 224 225 226 227 \n",
      "256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 \n",
      "292 293 294 295 288 289 290 291 300 301 302 303 296 297 298 299 308 309 310 311 304 305 306 307 316 317 318 319 312 313 314 315 \n",
      "328 329 330 331 332 333 334 335 320 321 322 323 324 325 326 327 344 345 346 347 348 349 350 351 336 337 338 339 340 341 342 343 \n",
      "364 365 366 367 360 361 362 363 356 357 358 359 352 353 354 355 380 381 382 383 376 377 378 379 372 373 374 375 368 369 370 371 \n",
      "400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 \n",
      "436 437 438 439 432 433 434 435 444 445 446 447 440 441 442 443 420 421 422 423 416 417 418 419 428 429 430 431 424 425 426 427 \n",
      "472 473 474 475 476 477 478 479 464 465 466 467 468 469 470 471 456 457 458 459 460 461 462 463 448 449 450 451 452 453 454 455 \n",
      "508 509 510 511 504 505 506 507 500 501 502 503 496 497 498 499 492 493 494 495 488 489 490 491 484 485 486 487 480 481 482 483 \n",
      "512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 \n",
      "548 549 550 551 544 545 546 547 556 557 558 559 552 553 554 555 564 565 566 567 560 561 562 563 572 573 574 575 568 569 570 571 \n",
      "584 585 586 587 588 589 590 591 576 577 578 579 580 581 582 583 600 601 602 603 604 605 606 607 592 593 594 595 596 597 598 599 \n",
      "620 621 622 623 616 617 618 619 612 613 614 615 608 609 610 611 636 637 638 639 632 633 634 635 628 629 630 631 624 625 626 627 \n",
      "656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 \n",
      "692 693 694 695 688 689 690 691 700 701 702 703 696 697 698 699 676 677 678 679 672 673 674 675 684 685 686 687 680 681 682 683 \n",
      "728 729 730 731 732 733 734 735 720 721 722 723 724 725 726 727 712 713 714 715 716 717 718 719 704 705 706 707 708 709 710 711 \n",
      "764 765 766 767 760 761 762 763 756 757 758 759 752 753 754 755 748 749 750 751 744 745 746 747 740 741 742 743 736 737 738 739 \n",
      "768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 \n",
      "804 805 806 807 800 801 802 803 812 813 814 815 808 809 810 811 820 821 822 823 816 817 818 819 828 829 830 831 824 825 826 827 \n",
      "840 841 842 843 844 845 846 847 832 833 834 835 836 837 838 839 856 857 858 859 860 861 862 863 848 849 850 851 852 853 854 855 \n",
      "876 877 878 879 872 873 874 875 868 869 870 871 864 865 866 867 892 893 894 895 888 889 890 891 884 885 886 887 880 881 882 883 \n",
      "912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 \n",
      "948 949 950 951 944 945 946 947 956 957 958 959 952 953 954 955 932 933 934 935 928 929 930 931 940 941 942 943 936 937 938 939 \n",
      "984 985 986 987 988 989 990 991 976 977 978 979 980 981 982 983 968 969 970 971 972 973 974 975 960 961 962 963 964 965 966 967 \n",
      "1020 1021 1022 1023 1016 1017 1018 1019 1012 1013 1014 1015 1008 1009 1010 1011 1004 1005 1006 1007 1000 1001 1002 1003 996 997 998 999 992 993 994 995 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <assert.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cudaTypedefs.h> \n",
    "\n",
    "// Modern C++ CUDA headers for TMA\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "\n",
    "// Namespaces for cleaner code\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace cde = cuda::device::experimental;\n",
    "\n",
    "// --- Helper Macros ---\n",
    "#define CUDA_CHECK(call) \\\n",
    "    { \\\n",
    "        cudaError_t err = (call); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n",
    "                    cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "            exit(err); \\\n",
    "        } \\\n",
    "    }\n",
    "\n",
    "// --- Driver API Helper ---\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "    cudaDriverEntryPointQueryResult driver_status;\n",
    "    void* ptr = nullptr;\n",
    "    CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "    return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(ptr);\n",
    "}\n",
    "\n",
    "// --- Constants ---\n",
    "constexpr uint32_t N = 32; \n",
    "constexpr uint32_t BN = 32;\n",
    "constexpr uint32_t rank = 2; \n",
    "\n",
    "// --- Kernel ---\n",
    "__global__ void tma_kernel(float* A, const __grid_constant__ CUtensorMap tensor_map)\n",
    "{\n",
    "    // 1. Shared Memory Buffer (Aligned for TMA)\n",
    "    __shared__ alignas(128) float smem_buffer[BN][BN];\n",
    "    \n",
    "    // 2. MBarrier for synchronization\n",
    "    #pragma nv_diag_suppress static_var_with_dynamic_init\n",
    "    __shared__ barrier bar;\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        init(&bar, blockDim.x);\n",
    "        cde::fence_proxy_async_shared_cta();\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    // 3. TMA Load (Global -> Shared)\n",
    "    barrier::arrival_token token;\n",
    "    if (threadIdx.x == 0) {\n",
    "        // Loads the data from Global to Shared. \n",
    "        // If swizzling is enabled in the map, it happens here during the write to SMEM.\n",
    "        cde::cp_async_bulk_tensor_2d_global_to_shared(&smem_buffer, &tensor_map, 0, 0, bar);\n",
    "        token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(smem_buffer));\n",
    "    } else {\n",
    "        token = bar.arrive();\n",
    "    }\n",
    "    bar.wait(std::move(token));\n",
    "\n",
    "    // 4. Fence to ensure the TMA writes are visible to the CUDA cores\n",
    "    cde::fence_proxy_async_shared_cta();\n",
    "    __syncthreads();\n",
    "\n",
    "    // 5. Manual Write Back (Shared -> Global)\n",
    "    // We do NOT use TMA Store here. We use standard threads to read the SMEM \n",
    "    // linearly (as per C++ layout) and write to GMEM.\n",
    "    // If TMA swizzled the data in step 3, smem_buffer[row][col] will contain\n",
    "    // the data that physically resides at that offset, revealing the pattern.\n",
    "    \n",
    "    // Each thread handles one row (BN elements)\n",
    "    if (threadIdx.x < BN) {\n",
    "        for (int i = 0; i < BN; ++i) {\n",
    "            A[threadIdx.x * BN + i] = smem_buffer[threadIdx.x][i];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // 6. Cleanup Barrier\n",
    "    if (threadIdx.x == 0) {\n",
    "        (&bar)->~barrier();\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    float *A_h, *A_d; \n",
    "    size_t size_tensor = N * N * sizeof(float);\n",
    "    \n",
    "    // Allocation\n",
    "    CUDA_CHECK(cudaHostAlloc(&A_h, size_tensor, cudaHostAllocDefault)); \n",
    "    CUDA_CHECK(cudaMalloc(&A_d, size_tensor)); \n",
    "\n",
    "    // Initialize Host Data\n",
    "    for(int i=0; i<N*N; i++) A_h[i] = (float)i;\n",
    "    CUDA_CHECK(cudaMemcpy(A_d, A_h, size_tensor, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Tensor Map Configuration\n",
    "    uint64_t size[rank] = {N, N}; // Elements\n",
    "    uint64_t stride[rank-1] = {N * sizeof(float)}; // Bytes\n",
    "    uint32_t box_size[rank] = {BN, BN}; \n",
    "    uint32_t elem_stride[rank] = {1, 1};\n",
    "\n",
    "    // --- FIX: Pass the device pointer directly, cast to void* ---\n",
    "    void *tensor_ptr = (void*)A_d; \n",
    "\n",
    "    CUtensorMap tensor_map{};\n",
    "    auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled();\n",
    "\n",
    "    CUresult res = cuTensorMapEncodeTiled(\n",
    "        &tensor_map,\n",
    "        // Improved: Use FLOAT32 to match your data\n",
    "        CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT32, \n",
    "        rank,\n",
    "        tensor_ptr, // <--- The Fix\n",
    "        size,\n",
    "        stride,\n",
    "        box_size,\n",
    "        elem_stride,\n",
    "        CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "        CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B,\n",
    "        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "    );\n",
    "\n",
    "    if (res != CUDA_SUCCESS) {\n",
    "        printf(\"Tensor Map Encode Failed!\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Launch\n",
    "    tma_kernel<<<1, 32>>>(A_d, tensor_map);\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    \n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "\n",
    "    // Verify\n",
    "    CUDA_CHECK(cudaMemcpy(A_h, A_d, size_tensor, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"ALL elements :\\n\");\n",
    "    for (int i = 0; i < BN*BN; i++) {\n",
    "        if (i % 32 == 0)\n",
    "            printf(\"\\n\");\n",
    "        // Expected: Original Value (i) if no swizzle\n",
    "        printf(\"%d \", (int) A_h[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    cudaFreeHost(A_h); \n",
    "    cudaFree(A_d);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c6c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Swizzle Pattern Dump (BN=8) ---\n",
      "Value at A[i] implies which original index landed there.\n",
      "\n",
      "   0,    1,    2,    3,    4,    5,    6,    7, \n",
      "   8,    9,   10,   11,   12,   13,   14,   15, \n",
      "  16,   17,   18,   19,   20,   21,   22,   23, \n",
      "  24,   25,   26,   27,   28,   29,   30,   31, \n",
      "  36,   37,   38,   39,   32,   33,   34,   35, \n",
      "  44,   45,   46,   47,   40,   41,   42,   43, \n",
      "  52,   53,   54,   55,   48,   49,   50,   51, \n",
      "  60,   61,   62,   63,   56,   57,   58,   59, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <assert.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cudaTypedefs.h> \n",
    "\n",
    "// Modern C++ CUDA headers for TMA\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "\n",
    "// Namespaces for cleaner code\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace cde = cuda::device::experimental;\n",
    "\n",
    "// --- Helper Macros ---\n",
    "#define CUDA_CHECK(call) \\\n",
    "    { \\\n",
    "        cudaError_t err = (call); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n",
    "                    cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "            exit(err); \\\n",
    "        } \\\n",
    "    }\n",
    "\n",
    "// --- Driver API Helper ---\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "    cudaDriverEntryPointQueryResult driver_status;\n",
    "    void* ptr = nullptr;\n",
    "    CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "    return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(ptr);\n",
    "}\n",
    "\n",
    "// --- Constants ---\n",
    "constexpr uint32_t N = 8; \n",
    "// BN can be changed, but must ensure BN*BN <= 1024 (Max threads per block)\n",
    "constexpr uint32_t BN = 8; \n",
    "constexpr uint32_t rank = 2; \n",
    "\n",
    "static_assert(BN * BN <= 1024, \"BN too large for single block 1:1 mapping\");\n",
    "\n",
    "// --- Kernel ---\n",
    "__global__ void tma_kernel(float* A, const __grid_constant__ CUtensorMap tensor_map)\n",
    "{\n",
    "    // 1. Shared Memory Buffer (Aligned for TMA)\n",
    "    __shared__ alignas(128) float smem_buffer[BN][BN];\n",
    "    \n",
    "    // 2. MBarrier for synchronization\n",
    "    #pragma nv_diag_suppress static_var_with_dynamic_init\n",
    "    __shared__ barrier bar;\n",
    "\n",
    "    // Initialize barrier with all threads in the block\n",
    "    if (threadIdx.x == 0) {\n",
    "        init(&bar, blockDim.x);\n",
    "        cde::fence_proxy_async_shared_cta();\n",
    "    }\n",
    "    __syncthreads();\n",
    "\n",
    "    // 3. TMA Load (Global -> Shared)\n",
    "    // Only thread 0 issues the TMA command\n",
    "    barrier::arrival_token token;\n",
    "    if (threadIdx.x == 0) {\n",
    "        // Loads the data from Global to Shared. \n",
    "        // Swizzling (128B) happens here during the write to SMEM banks.\n",
    "        cde::cp_async_bulk_tensor_2d_global_to_shared(&smem_buffer, &tensor_map, 0, 0, bar);\n",
    "        token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(smem_buffer));\n",
    "    } else {\n",
    "        token = bar.arrive();\n",
    "    }\n",
    "    bar.wait(std::move(token));\n",
    "\n",
    "    // 4. Fence to ensure the TMA writes are visible to the CUDA cores\n",
    "    cde::fence_proxy_async_shared_cta();\n",
    "    __syncthreads();\n",
    "\n",
    "    // 5. Manual Write Back (Shared -> Global) - 1:1 Mapping\n",
    "    // We treat the SMEM as a flat linear buffer via standard C++ indices.\n",
    "    // Because TMA wrote physically swizzled data, but we read logically linear data,\n",
    "    // the output 'A' will contain the swizzled pattern.\n",
    "    \n",
    "    int tid = threadIdx.x;\n",
    "    int row = tid / BN;\n",
    "    int col = tid % BN;\n",
    "\n",
    "    // Boundary check usually not needed if BN*BN == blockDim.x, but good practice\n",
    "    if (row < BN && col < BN) {\n",
    "        A[tid] = smem_buffer[row][col];\n",
    "    }\n",
    "\n",
    "    // 6. Cleanup Barrier\n",
    "    // Wait for all threads to finish reading before destroying barrier\n",
    "    __syncthreads();\n",
    "    if (threadIdx.x == 0) {\n",
    "        (&bar)->~barrier();\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    float *A_h, *A_d; \n",
    "    size_t size_tensor = N * N * sizeof(float);\n",
    "    \n",
    "    // Allocation\n",
    "    CUDA_CHECK(cudaHostAlloc(&A_h, size_tensor, cudaHostAllocDefault)); \n",
    "    CUDA_CHECK(cudaMalloc(&A_d, size_tensor)); \n",
    "\n",
    "    // Initialize Host Data with sequential indices to easily track movement\n",
    "    for(int i=0; i<N*N; i++) A_h[i] = (float)i;\n",
    "    CUDA_CHECK(cudaMemcpy(A_d, A_h, size_tensor, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    // Tensor Map Configuration\n",
    "    uint64_t size[rank] = {N, N}; // Elements\n",
    "    uint64_t stride[rank-1] = {N * sizeof(float)}; // Bytes\n",
    "    uint32_t box_size[rank] = {BN, BN}; \n",
    "    uint32_t elem_stride[rank] = {1, 1};\n",
    "\n",
    "    void *tensor_ptr = (void*)A_d; \n",
    "\n",
    "    CUtensorMap tensor_map{};\n",
    "    auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled();\n",
    "\n",
    "    CUresult res = cuTensorMapEncodeTiled(\n",
    "        &tensor_map,\n",
    "        CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT32, \n",
    "        rank,\n",
    "        tensor_ptr, \n",
    "        size,\n",
    "        stride,\n",
    "        box_size,\n",
    "        elem_stride,\n",
    "        CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "        // ENABLE SWIZZLE HERE to detect the pattern\n",
    "        CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_32B,\n",
    "        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "    );\n",
    "\n",
    "    if (res != CUDA_SUCCESS) {\n",
    "        printf(\"Tensor Map Encode Failed!\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Launch with exactly enough threads for the tile\n",
    "    // 32*32 = 1024 threads\n",
    "    tma_kernel<<<1, BN * BN>>>(A_d, tensor_map);\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    \n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "\n",
    "    // Verify\n",
    "    CUDA_CHECK(cudaMemcpy(A_h, A_d, size_tensor, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    printf(\"--- Swizzle Pattern Dump (BN=%d) ---\\n\", BN);\n",
    "    printf(\"Value at A[i] implies which original index landed there.\\n\");\n",
    "    for (int i = 0; i < BN*BN; i++) {\n",
    "        if (i % BN == 0)\n",
    "            printf(\"\\n\");\n",
    "        \n",
    "        printf(\"%4d, \", (int)A_h[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    cudaFreeHost(A_h); \n",
    "    cudaFree(A_d);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e30a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 132, 133, 134, 135, 128, 129, 130, 131, 140, 141, 142, 143, 136, 137, 138, 139, 148, 149, 150, 151, 144, 145, 146, 147, 156, 157, 158, 159, 152, 153, 154, 155, 164, 165, 166, 167, 160, 161, 162, 163, 172, 173, 174, 175, 168, 169, 170, 171, 180, 181, 182, 183, 176, 177, 178, 179, 188, 189, 190, 191, 184, 185, 186, 187, 196, 197, 198, 199, 192, 193, 194, 195, 204, 205, 206, 207, 200, 201, 202, 203, 212, 213, 214, 215, 208, 209, 210, 211, 220, 221, 222, 223, 216, 217, 218, 219, 228, 229, 230, 231, 224, 225, 226, 227, 236, 237, 238, 239, 232, 233, 234, 235, 244, 245, 246, 247, 240, 241, 242, 243, 252, 253, 254, 255, 248, 249, 250, 251, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 388, 389, 390, 391, 384, 385, 386, 387, 396, 397, 398, 399, 392, 393, 394, 395, 404, 405, 406, 407, 400, 401, 402, 403, 412, 413, 414, 415, 408, 409, 410, 411, 420, 421, 422, 423, 416, 417, 418, 419, 428, 429, 430, 431, 424, 425, 426, 427, 436, 437, 438, 439, 432, 433, 434, 435, 444, 445, 446, 447, 440, 441, 442, 443, 452, 453, 454, 455, 448, 449, 450, 451, 460, 461, 462, 463, 456, 457, 458, 459, 468, 469, 470, 471, 464, 465, 466, 467, 476, 477, 478, 479, 472, 473, 474, 475, 484, 485, 486, 487, 480, 481, 482, 483, 492, 493, 494, 495, 488, 489, 490, 491, 500, 501, 502, 503, 496, 497, 498, 499, 508, 509, 510, 511, 504, 505, 506, 507, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 644, 645, 646, 647, 640, 641, 642, 643, 652, 653, 654, 655, 648, 649, 650, 651, 660, 661, 662, 663, 656, 657, 658, 659, 668, 669, 670, 671, 664, 665, 666, 667, 676, 677, 678, 679, 672, 673, 674, 675, 684, 685, 686, 687, 680, 681, 682, 683, 692, 693, 694, 695, 688, 689, 690, 691, 700, 701, 702, 703, 696, 697, 698, 699, 708, 709, 710, 711, 704, 705, 706, 707, 716, 717, 718, 719, 712, 713, 714, 715, 724, 725, 726, 727, 720, 721, 722, 723, 732, 733, 734, 735, 728, 729, 730, 731, 740, 741, 742, 743, 736, 737, 738, 739, 748, 749, 750, 751, 744, 745, 746, 747, 756, 757, 758, 759, 752, 753, 754, 755, 764, 765, 766, 767, 760, 761, 762, 763, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 900, 901, 902, 903, 896, 897, 898, 899, 908, 909, 910, 911, 904, 905, 906, 907, 916, 917, 918, 919, 912, 913, 914, 915, 924, 925, 926, 927, 920, 921, 922, 923, 932, 933, 934, 935, 928, 929, 930, 931, 940, 941, 942, 943, 936, 937, 938, 939, 948, 949, 950, 951, 944, 945, 946, 947, 956, 957, 958, 959, 952, 953, 954, 955, 964, 965, 966, 967, 960, 961, 962, 963, 972, 973, 974, 975, 968, 969, 970, 971, 980, 981, 982, 983, 976, 977, 978, 979, 988, 989, 990, 991, 984, 985, 986, 987, 996, 997, 998, 999, 992, 993, 994, 995, 1004, 1005, 1006, 1007, 1000, 1001, 1002, 1003, 1012, 1013, 1014, 1015, 1008, 1009, 1010, 1011, 1020, 1021, 1022, 1023, 1016, 1017, 1018, 1019, \n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <assert.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cudaTypedefs.h> \n",
    "\n",
    "// Modern C++ CUDA headers for TMA\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "\n",
    "// Namespaces for cleaner code\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace cde = cuda::device::experimental;\n",
    "\n",
    "// --- Helper Macros ---\n",
    "#define CUDA_CHECK(call) \\\n",
    "    { \\\n",
    "        cudaError_t err = (call); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n",
    "                    cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "            exit(err); \\\n",
    "        } \\\n",
    "    }\n",
    "\n",
    "// --- Driver API Helper ---\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "    cudaDriverEntryPointQueryResult driver_status;\n",
    "    void* ptr = nullptr;\n",
    "    CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "    return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(ptr);\n",
    "}\n",
    "\n",
    "// --- constants --- \n",
    "\n",
    "constexpr int byte_aligner = 128; \n",
    "constexpr uint32_t M = 32; \n",
    "constexpr uint32_t N = 32; \n",
    "constexpr uint32_t BM = 16; \n",
    "constexpr uint32_t BN = 8;\n",
    "constexpr uint32_t GM = M/BM; \n",
    "constexpr uint32_t GN = N/BN;\n",
    "constexpr uint32_t rank = 2; \n",
    "\n",
    "static_assert(BM*BN <= 1024, \"too many threads\");\n",
    "\n",
    "constexpr uint64_t tensor_shape[rank] = {N,M}; //fast dimension first \n",
    "constexpr uint64_t tensor_stride[rank-1] = {N*sizeof(float)}; //number of bytes in the fast dimension is the stride\n",
    "constexpr uint32_t smem_box_shape[rank] = {BN,BM}; \n",
    "constexpr uint32_t element_stride[rank] = {1,1};\n",
    "constexpr size_t gmem_tensor_size = M*N*sizeof(float);\n",
    "\n",
    "__global__ void tma_kernel(float* A, const __grid_constant__ CUtensorMap tensor_map)\n",
    "{\n",
    "    uint x = blockIdx.x*blockDim.x; \n",
    "    uint y = blockIdx.y*blockDim.y;\n",
    "    __shared__ alignas(byte_aligner) float As[BM][BN];  // declared aligned shared memory \n",
    "\n",
    "    #pragma nv_diag_suppress static_var_with_dynamic_init // tells compiler hey chill the fuck out we know whats good.\n",
    "    __shared__ barrier bar;\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        init(&bar, BM*BN);\n",
    "        cde::fence_proxy_async_shared_cta(); // special proxy that waits and ensures that the TMA engine has visibility to the bar. \n",
    "    }\n",
    "    __syncthreads(); // ensure all thereads are all synced up before using the barrier (bootstrapped init)\n",
    "\n",
    "    barrier::arrival_token token;\n",
    "    if (threadIdx.x == 0)\n",
    "    {\n",
    "        // Loads the data from Global to Shared. \n",
    "        // Swizzling (128B) happens here during the write to SMEM banks.\n",
    "        cde::cp_async_bulk_tensor_2d_global_to_shared(&As, &tensor_map, x, y, bar);\n",
    "        token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(As));\n",
    "    } \n",
    "    else {\n",
    "        token = bar.arrive();\n",
    "    }\n",
    "\n",
    "    bar.wait(std::move(token));\n",
    "    cde::fence_proxy_async_shared_cta();\n",
    "    __syncthreads();\n",
    "\n",
    "    uint smem_col = threadIdx.x; \n",
    "    uint smem_row = threadIdx.y; \n",
    "    uint gmem_row = y + smem_row;\n",
    "    uint gmem_col = x + smem_col;\n",
    "    A[gmem_row*N + gmem_col] = As[smem_row][smem_col];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    float* A_h, *A_d; \n",
    "    cudaHostAlloc(&A_h, gmem_tensor_size, cudaHostAllocDefault); \n",
    "    cudaMalloc(&A_d, gmem_tensor_size); \n",
    "    for (int i = 0; i < M*N; i++)\n",
    "    {\n",
    "        A_h[i] = (float) i; \n",
    "    }\n",
    "    cudaMemcpy(A_d, A_h, gmem_tensor_size, cudaMemcpyHostToDevice); \n",
    "    auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled();\n",
    "    CUtensorMap tensor_map{};\n",
    "    void *tensor_ptr = (void*)A_d; \n",
    "    CUresult res = cuTensorMapEncodeTiled(\n",
    "        &tensor_map,\n",
    "        CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT32, \n",
    "        rank,\n",
    "        tensor_ptr, \n",
    "        tensor_shape,\n",
    "        tensor_stride,\n",
    "        smem_box_shape,\n",
    "        element_stride,\n",
    "        CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "        CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_32B,\n",
    "        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "    );\n",
    "\n",
    "    if (res != CUDA_SUCCESS) {\n",
    "        printf(\"Tensor Map Encode Failed!\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    dim3 grid(GN, GM); \n",
    "    dim3 block(BN, BM);\n",
    "    tma_kernel<<<grid, block>>>(A_d, tensor_map);\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaMemcpy(A_h, A_d, gmem_tensor_size, cudaMemcpyDeviceToHost);\n",
    "    for (int i = 0; i < N*M; i++) printf(\"%d, \", (int) A_h[i]);\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a558fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, \n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <assert.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda.h>\n",
    "#include <cudaTypedefs.h> \n",
    "#include <cuda_bf16.h>\n",
    "\n",
    "// Modern C++ CUDA headers for TMA\n",
    "#include <cuda/barrier>\n",
    "#include <cuda/ptx>\n",
    "\n",
    "// Namespaces for cleaner code\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace cde = cuda::device::experimental;\n",
    "\n",
    "// --- Helper Macros ---\n",
    "#define CUDA_CHECK(call) \\\n",
    "    { \\\n",
    "        cudaError_t err = (call); \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA Error: %s at %s:%d\\n\", \\\n",
    "                    cudaGetErrorString(err), __FILE__, __LINE__); \\\n",
    "            exit(err); \\\n",
    "        } \\\n",
    "    }\n",
    "\n",
    "// --- Driver API Helper ---\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "    cudaDriverEntryPointQueryResult driver_status;\n",
    "    void* ptr = nullptr;\n",
    "    CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "    return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(ptr);\n",
    "}\n",
    "\n",
    "// --- constants --- \n",
    "\n",
    "constexpr int byte_aligner = 1024; \n",
    "constexpr uint32_t M = 32; \n",
    "constexpr uint32_t N = 32; \n",
    "constexpr uint32_t BM = 16; \n",
    "constexpr uint32_t BN = 16;\n",
    "constexpr uint32_t GM = M/BM; \n",
    "constexpr uint32_t GN = N/BN;\n",
    "constexpr uint32_t rank = 2; \n",
    "\n",
    "static_assert(BM*BN/2 <= 1024, \"too many threads\");\n",
    "\n",
    "constexpr uint64_t tensor_shape[rank] = {N,M}; //fast dimension first \n",
    "constexpr uint64_t tensor_stride[rank-1] = {N*sizeof(__nv_bfloat16)}; //number of bytes in the fast dimension is the stride\n",
    "constexpr uint32_t smem_box_shape[rank] = {BN,BM}; \n",
    "constexpr uint32_t element_stride[rank] = {1,1};\n",
    "constexpr size_t gmem_tensor_size = M*N*sizeof(__nv_bfloat16);\n",
    "\n",
    "__global__ void tma_kernel(__nv_bfloat16* A, const __grid_constant__ CUtensorMap tensor_map)\n",
    "{\n",
    "    uint x = blockIdx.x*blockDim.x; \n",
    "    uint y = blockIdx.y*blockDim.y;\n",
    "    __shared__ alignas(byte_aligner) __nv_bfloat16 As[BM][BN];  // declared aligned shared memory \n",
    "\n",
    "    #pragma nv_diag_suppress static_var_with_dynamic_init // tells compiler hey chill the fuck out we know whats good.\n",
    "    __shared__ barrier bar;\n",
    "\n",
    "    if (threadIdx.x == 0) {\n",
    "        init(&bar, BM*BN);\n",
    "        cde::fence_proxy_async_shared_cta(); // special proxy that waits and ensures that the TMA engine has visibility to the bar. \n",
    "    }\n",
    "    __syncthreads(); // ensure all thereads are all synced up before using the barrier (bootstrapped init)\n",
    "\n",
    "    barrier::arrival_token token;\n",
    "    if (threadIdx.x == 0)\n",
    "    {\n",
    "        // Loads the data from Global to Shared. \n",
    "        // Swizzling (128B) happens here during the write to SMEM banks.\n",
    "        cde::cp_async_bulk_tensor_2d_global_to_shared(&As, &tensor_map, x, y, bar);\n",
    "        token = cuda::device::barrier_arrive_tx(bar, 1, sizeof(As));\n",
    "    } \n",
    "    else {\n",
    "        token = bar.arrive();\n",
    "    }\n",
    "\n",
    "    bar.wait(std::move(token));\n",
    "    cde::fence_proxy_async_shared_cta();\n",
    "    __syncthreads();\n",
    "\n",
    "    uint smem_col = threadIdx.x; \n",
    "    uint smem_row = threadIdx.y; \n",
    "    uint gmem_row = y + smem_row;\n",
    "    uint gmem_col = x + smem_col;\n",
    "    A[gmem_row*N + gmem_col] = As[smem_row][smem_col];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    __nv_bfloat16 * A_h, *A_d; \n",
    "    cudaHostAlloc(&A_h, gmem_tensor_size, cudaHostAllocDefault); \n",
    "    cudaMalloc(&A_d, gmem_tensor_size); \n",
    "    for (int i = 0; i < M*N; i++)\n",
    "    {\n",
    "        A_h[i] = __float2bfloat16((float) (i% (BN*BM))); \n",
    "    }\n",
    "    cudaMemcpy(A_d, A_h, gmem_tensor_size, cudaMemcpyHostToDevice); \n",
    "    auto cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled();\n",
    "    CUtensorMap tensor_map{};\n",
    "    void *tensor_ptr = (void*)A_d; \n",
    "    CUresult res = cuTensorMapEncodeTiled(\n",
    "        &tensor_map,\n",
    "        CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_BFLOAT16, \n",
    "        rank,\n",
    "        tensor_ptr, \n",
    "        tensor_shape,\n",
    "        tensor_stride,\n",
    "        smem_box_shape,\n",
    "        element_stride,\n",
    "        CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "        CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B,\n",
    "        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "    );\n",
    "\n",
    "    if (res != CUDA_SUCCESS) {\n",
    "        printf(\"Tensor Map Encode Failed!\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    dim3 grid(GN, GM); \n",
    "    dim3 block(BN, BM);\n",
    "    tma_kernel<<<grid, block>>>(A_d, tensor_map);\n",
    "    cudaDeviceSynchronize();\n",
    "    cudaMemcpy(A_h, A_d, gmem_tensor_size, cudaMemcpyDeviceToHost);\n",
    "    for (int i = 0; i < N*M; i++) printf(\"%d, \", (int) A_h[i]);\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9508cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyLayouts import Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f438a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
